#!/usr/bin/env python3
# -*- coding: utf-8 -*-

# ========================================================================
# ØªØ­Ø¯ÙŠ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø°Ø§ØªÙŠ (Ù„Ù„Ù…ØªØ®ØµØµÙŠÙ†)
# Ø§Ù„Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø«Ø§Ù†ÙŠ: Ù‡Ø¨ÙˆØ· Ø§Ù„Ù‚Ù…Ø± (Lunar Lander)
# Ø§Ù„ØµØ¹ÙˆØ¨Ø©: Ù…ØªÙˆØ³Ø·
# 
# Ø§Ø³Ù… Ø§Ù„Ù…Ù„Ù: lunar_lander_challenge.py
# ØªØ§Ø±ÙŠØ® Ø§Ù„Ø¥Ù†Ø´Ø§Ø¡: 2025
# ========================================================================

"""
 ÙˆØµÙ Ø§Ù„ØªØ­Ø¯ÙŠ:
--------------
Ù…Ø±ÙƒØ¨Ø© ÙØ¶Ø§Ø¦ÙŠØ© ØªØ­Ø§ÙˆÙ„ Ø§Ù„Ù‡Ø¨ÙˆØ· Ø¨Ø£Ù…Ø§Ù† Ø¹Ù„Ù‰ Ø³Ø·Ø­ Ø§Ù„Ù‚Ù…Ø± Ø¨ÙŠÙ† Ø§Ù„Ø¹Ù„Ù…ÙŠÙ†.
ÙŠØ¬Ø¨ Ø§Ù„ØªØ­ÙƒÙ… ÙÙŠ Ø§Ù„Ù…Ø­Ø±ÙƒØ§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© ÙˆØ§Ù„Ø¬Ø§Ù†Ø¨ÙŠØ© Ù„Ù„Ù‡Ø¨ÙˆØ· Ø§Ù„Ø¢Ù…Ù†.

Ø§Ù„ØªØ­Ø¯ÙŠØ§Øª:
- Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„ÙˆÙ‚ÙˆØ¯ Ø§Ù„Ù…Ø­Ø¯ÙˆØ¯
- Ø§Ù„ØªØ­ÙƒÙ… ÙÙŠ Ø§Ù„Ø³Ø±Ø¹Ø© ÙˆØ§Ù„Ø¯ÙˆØ±Ø§Ù†
- Ø§Ù„Ù‡Ø¨ÙˆØ· ÙÙŠ Ø§Ù„Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ø¢Ù…Ù†Ø©
- Ø¹Ø¯Ù… Ø§Ù„Ø§ØµØ·Ø¯Ø§Ù… Ø¨Ù‚ÙˆØ©

 Ø§Ù„Ù‚ÙˆØ§Ù†ÙŠÙ† ÙˆØ§Ù„Ù‚ÙŠÙˆØ¯:
-------------------
1. ÙŠØ¬Ø¨ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© Q-Learning ÙÙ‚Ø·
2. Ù„Ø§ ÙŠØ³Ù…Ø­ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Deep Learning Ø£Ùˆ Neural Networks
3. ÙŠØ¬Ø¨ ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø­Ø§Ù„Ø§Øª (State Discretization) - 8 Ø£Ø¨Ø¹Ø§Ø¯ Ù…Ø³ØªÙ…Ø±Ø©
4. Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ù…Ø³Ù…ÙˆØ­Ø©: 
   - 0: Ù„Ø§ Ø´ÙŠØ¡
   - 1: Ù…Ø­Ø±Ùƒ Ø£ÙŠØ³Ø±
   - 2: Ù…Ø­Ø±Ùƒ Ø±Ø¦ÙŠØ³ÙŠ
   - 3: Ù…Ø­Ø±Ùƒ Ø£ÙŠÙ…Ù†
5. Ø§Ù„Ù†Ø¬Ø§Ø­ = Ø§Ù„Ù‡Ø¨ÙˆØ· Ø§Ù„Ø¢Ù…Ù† Ø¨Ù…ÙƒØ§ÙØ£Ø© Ø¥ÙŠØ¬Ø§Ø¨ÙŠØ©

 Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„ØªÙ‚ÙŠÙŠÙ…:
------------------
- Ù…ÙƒØ§ÙØ£Ø© Ø§Ù„Ù‡Ø¨ÙˆØ· Ø§Ù„Ø¢Ù…Ù†: +100 Ø¥Ù„Ù‰ +140
- Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ÙˆÙ‚ÙˆØ¯: -0.3 Ù„ÙƒÙ„ Ù…Ø­Ø±Ùƒ
- ØªØ­Ø·Ù… Ø§Ù„Ù…Ø±ÙƒØ¨Ø©: -100
- Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ: Ù…ØªÙˆØ³Ø· Ø¢Ø®Ø± 100 Ø­Ù„Ù‚Ø©

 ØªÙ†Ø¨ÙŠÙ‡Ø§Øª Ù‡Ø§Ù…Ø©:
-----------------
- Ù„Ø§ ØªÙ‚Ù… Ø¨ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ¦Ø© Ø£Ùˆ Ù‚ÙˆØ§Ù†ÙŠÙ† Ø§Ù„Ù…ÙƒØ§ÙØ¢Øª
- ÙŠÙ…ÙƒÙ†Ùƒ ÙÙ‚Ø· ØªØ¹Ø¯ÙŠÙ„ Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„ØªØ¹Ù„Ù… ÙˆØ·Ø±ÙŠÙ‚Ø© Ø§Ù„ØªÙ‚Ø³ÙŠÙ…
- Ø§Ù„ØªØ­Ø¯ÙŠ Ø£ØµØ¹Ø¨ Ù…Ù† Mountain Car - ÙŠØ­ØªØ§Ø¬ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ù…ØªÙ‚Ø¯Ù…Ø©
"""

# ========================================================================
# 1ï¸âƒ£ Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
# ========================================================================

import numpy as np
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation
from IPython.display import HTML
import gymnasium as gym
from collections import defaultdict
import warnings
warnings.filterwarnings('ignore')

# ========================================================================
# 2ï¸âƒ£ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¨ÙŠØ¦Ø© (Ù„Ø§ ØªØ¹Ø¯Ù„ Ù‡Ø°Ø§ Ø§Ù„Ù‚Ø³Ù…!)
# ========================================================================

class LunarLanderChallenge:
    """
    Ø¨ÙŠØ¦Ø© ØªØ­Ø¯ÙŠ Lunar Lander
     Ù…Ù…Ù†ÙˆØ¹ Ø§Ù„ØªØ¹Ø¯ÙŠÙ„ Ø¹Ù„Ù‰ Ù‡Ø°Ø§ Ø§Ù„ÙƒÙ„Ø§Ø³!
    
    Ø§Ù„Ø­Ø§Ù„Ø© (8 Ø£Ø¨Ø¹Ø§Ø¯):
    - x: Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ø£ÙÙ‚ÙŠ
    - y: Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ø¹Ù…ÙˆØ¯ÙŠ
    - vx: Ø§Ù„Ø³Ø±Ø¹Ø© Ø§Ù„Ø£ÙÙ‚ÙŠØ©
    - vy: Ø§Ù„Ø³Ø±Ø¹Ø© Ø§Ù„Ø¹Ù…ÙˆØ¯ÙŠØ©
    - angle: Ø²Ø§ÙˆÙŠØ© Ø§Ù„Ø¯ÙˆØ±Ø§Ù†
    - angular_velocity: Ø³Ø±Ø¹Ø© Ø§Ù„Ø¯ÙˆØ±Ø§Ù†
    - leg1_contact: Ù…Ù„Ø§Ù…Ø³Ø© Ø§Ù„Ø³Ø§Ù‚ Ø§Ù„ÙŠØ³Ø±Ù‰ Ù„Ù„Ø£Ø±Ø¶
    - leg2_contact: Ù…Ù„Ø§Ù…Ø³Ø© Ø§Ù„Ø³Ø§Ù‚ Ø§Ù„ÙŠÙ…Ù†Ù‰ Ù„Ù„Ø£Ø±Ø¶
    """
    
    def __init__(self):
        self.env = gym.make('LunarLander-v2')
        self.state_bins = None
        
    def setup_discretization(self, bins_per_dimension=10):
        """
        Ø¥Ø¹Ø¯Ø§Ø¯ ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„Ù…Ø³ØªÙ…Ø±Ø©
        
        Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª:
        -----------
        bins_per_dimension: Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ù‚Ø³Ø§Ù… Ù„ÙƒÙ„ Ø¨ÙØ¹Ø¯ (Ø§ÙØªØ±Ø§Ø¶ÙŠ: 10)
        
        Ù…Ù„Ø§Ø­Ø¸Ø©: Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø¹Ø¯Ø¯ = Ø¯Ù‚Ø© Ø£Ø¹Ù„Ù‰ Ù„ÙƒÙ† ÙˆÙ‚Øª ØªØ¯Ø±ÙŠØ¨ Ø£Ø·ÙˆÙ„
        """
        # Ø­Ø¯ÙˆØ¯ ØªÙ‚Ø±ÙŠØ¨ÙŠØ© Ù„ÙƒÙ„ Ø¨ÙØ¹Ø¯ Ù…Ù† Ø§Ù„Ø­Ø§Ù„Ø©
        self.state_bounds = [
            (-1.5, 1.5),    # x position
            (-1.5, 1.5),    # y position
            (-2.5, 2.5),    # x velocity
            (-2.5, 2.5),    # y velocity
            (-3.14, 3.14),  # angle
            (-5.0, 5.0),    # angular velocity
            (0, 1),         # leg 1 contact (binary)
            (0, 1)          # leg 2 contact (binary)
        ]
        
        self.state_bins = []
        for i, (low, high) in enumerate(self.state_bounds):
            if i >= 6:  # Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯ Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠØ© (Ø§Ù„Ø³Ø§Ù‚ÙŠÙ†)
                self.state_bins.append(np.array([0, 1]))
            else:
                self.state_bins.append(
                    np.linspace(low, high, bins_per_dimension)
                )
    
    def discretize_state(self, state):
        """ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø­Ø§Ù„Ø© Ø§Ù„Ù…Ø³ØªÙ…Ø±Ø© Ø¥Ù„Ù‰ Ù…Ù†ÙØµÙ„Ø©"""
        discrete_state = []
        
        for i, (s, bins) in enumerate(zip(state, self.state_bins)):
            # ØªÙ‚ÙŠÙŠØ¯ Ø§Ù„Ù‚ÙŠÙ…Ø© Ø¶Ù…Ù† Ø§Ù„Ø­Ø¯ÙˆØ¯
            s_clipped = np.clip(s, self.state_bounds[i][0], 
                               self.state_bounds[i][1])
            # ØªØ­ÙˆÙŠÙ„ Ù„ÙÙ‡Ø±Ø³ Ù…Ù†ÙØµÙ„
            idx = np.digitize(s_clipped, bins)
            discrete_state.append(idx)
        
        return tuple(discrete_state)
    
    def reset(self):
        """Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ† Ø§Ù„Ø¨ÙŠØ¦Ø©"""
        state, _ = self.env.reset()
        return self.discretize_state(state)
    
    def step(self, action):
        """
        ØªÙ†ÙÙŠØ° Ø®Ø·ÙˆØ© ÙÙŠ Ø§Ù„Ø¨ÙŠØ¦Ø©
        
        Ø§Ù„Ù…ÙƒØ§ÙØ¢Øª (Ù„Ø§ ÙŠÙ…ÙƒÙ† ØªØ¹Ø¯ÙŠÙ„Ù‡Ø§):
        - Ø§Ù„Ù‡Ø¨ÙˆØ· Ø§Ù„Ø¢Ù…Ù†: +100 Ø¥Ù„Ù‰ +140
        - Ø§Ù„ØªØ­Ø·Ù…: -100
        - Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø­Ø±ÙƒØ§Øª: -0.3 Ù„ÙƒÙ„ Ø¥Ø·Ø§Ø±
        - Ø§Ù„Ø­Ø±ÙƒØ© Ù†Ø­Ùˆ Ø§Ù„Ù‡Ø¨ÙˆØ·: Ù…ÙƒØ§ÙØ¢Øª ØªØ¯Ø±ÙŠØ¬ÙŠØ©
        """
        next_state, reward, terminated, truncated, info = self.env.step(action)
        done = terminated or truncated
        
        return self.discretize_state(next_state), reward, done, info
    
    def render(self):
        """Ø¹Ø±Ø¶ Ø§Ù„Ø¨ÙŠØ¦Ø©"""
        return self.env.render()
    
    def close(self):
        """Ø¥ØºÙ„Ø§Ù‚ Ø§Ù„Ø¨ÙŠØ¦Ø©"""
        self.env.close()


# ========================================================================
# 3ï¸âƒ£ Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© Q-Learning (ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„ØªØ¹Ø¯ÙŠÙ„ Ù‡Ù†Ø§!)
# ========================================================================

class QLearningAgent:
    """
    ÙˆÙƒÙŠÙ„ Q-Learning Ù„ØªØ­Ø¯ÙŠ Lunar Lander
    
     ÙŠÙ…ÙƒÙ†Ùƒ ØªØ¹Ø¯ÙŠÙ„:
    - Ù‚ÙŠÙ… Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª (learning_rate, discount_factor, etc.)
    - Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© epsilon decay
    - Ø·Ø±ÙŠÙ‚Ø© Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡
    
     Ù„Ø§ ÙŠÙ…ÙƒÙ†Ùƒ:
    - Ø§Ø³ØªØ®Ø¯Ø§Ù… Neural Networks
    - ØªØºÙŠÙŠØ± Ø§Ù„Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
    """
    
    def __init__(self, 
                 n_actions=4,
                 learning_rate=0.1,
                 discount_factor=0.99,
                 epsilon_start=1.0,
                 epsilon_end=0.01,
                 epsilon_decay=0.995):
        """
        Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„ØªØ¹Ø¯ÙŠÙ„:
        ---------------------------
        learning_rate: Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù… (alpha) - Ø¬Ø±Ø¨ Ù‚ÙŠÙ… Ø¨ÙŠÙ† 0.01 Ùˆ 0.3
        discount_factor: Ù…Ø¹Ø§Ù…Ù„ Ø§Ù„Ø®ØµÙ… (gamma) - Ø¬Ø±Ø¨ Ù‚ÙŠÙ… Ø¨ÙŠÙ† 0.95 Ùˆ 0.999
        epsilon_start: Ù‚ÙŠÙ…Ø© epsilon Ø§Ù„Ø§Ø¨ØªØ¯Ø§Ø¦ÙŠØ©
        epsilon_end: Ù‚ÙŠÙ…Ø© epsilon Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
        epsilon_decay: Ù…Ø¹Ø¯Ù„ ØªÙ†Ø§Ù‚Øµ epsilon
        """
        
        self.n_actions = n_actions
        self.learning_rate = learning_rate
        self.discount_factor = discount_factor
        self.epsilon = epsilon_start
        self.epsilon_end = epsilon_end
        self.epsilon_decay = epsilon_decay
        
        # Ø¬Ø¯ÙˆÙ„ Q Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… defaultdict
        self.q_table = defaultdict(lambda: np.zeros(n_actions))
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
        self.training_episodes = 0
    
    def get_action(self, state, training=True):
        """
        Ø§Ø®ØªÙŠØ§Ø± Ø¥Ø¬Ø±Ø§Ø¡ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… epsilon-greedy
        
        ÙŠÙ…ÙƒÙ†Ùƒ ØªØ¹Ø¯ÙŠÙ„ Ù‡Ø°Ù‡ Ø§Ù„Ø¯Ø§Ù„Ø© Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡!
        Ù…Ø«Ù„Ø§Ù‹: Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ø§Ø³ØªÙƒØ´Ø§Ù Ù…ØªÙ‚Ø¯Ù…Ø©
        """
        if training and np.random.random() < self.epsilon:
            # Ø§Ø³ØªÙƒØ´Ø§Ù: Ø§Ø®ØªÙŠØ§Ø± Ø¹Ø´ÙˆØ§Ø¦ÙŠ
            return np.random.randint(0, self.n_actions)
        else:
            # Ø§Ø³ØªØºÙ„Ø§Ù„: Ø§Ø®ØªÙŠØ§Ø± Ø£ÙØ¶Ù„ Ø¥Ø¬Ø±Ø§Ø¡
            q_values = self.q_table[state]
            # ÙÙŠ Ø­Ø§Ù„Ø© ØªØ³Ø§ÙˆÙŠ Ø§Ù„Ù‚ÙŠÙ…ØŒ Ø§Ø®ØªØ± Ø¹Ø´ÙˆØ§Ø¦ÙŠØ§Ù‹
            max_q = np.max(q_values)
            best_actions = np.where(q_values == max_q)[0]
            return np.random.choice(best_actions)
    
    def update(self, state, action, reward, next_state, done):
        """
        ØªØ­Ø¯ÙŠØ« Ø¬Ø¯ÙˆÙ„ Q Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¹Ø§Ø¯Ù„Ø© Q-Learning
        
        Q(s,a) = Q(s,a) + Î± * [r + Î³ * max(Q(s',a')) - Q(s,a)]
        """
        current_q = self.q_table[state][action]
        
        if done:
            # Ø¥Ø°Ø§ Ø§Ù†ØªÙ‡Øª Ø§Ù„Ø­Ù„Ù‚Ø©
            max_next_q = 0
        else:
            # Ø£Ù‚ØµÙ‰ Ù‚ÙŠÙ…Ø© Q Ù„Ù„Ø­Ø§Ù„Ø© Ø§Ù„ØªØ§Ù„ÙŠØ©
            max_next_q = np.max(self.q_table[next_state])
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„Ù…Ø³ØªÙ‡Ø¯ÙØ©
        target_q = reward + self.discount_factor * max_next_q
        
        # ØªØ­Ø¯ÙŠØ« Q
        new_q = current_q + self.learning_rate * (target_q - current_q)
        self.q_table[state][action] = new_q
    
    def decay_epsilon(self):
        """ØªÙ‚Ù„ÙŠÙ„ epsilon Ø¨Ø¹Ø¯ ÙƒÙ„ Ø­Ù„Ù‚Ø©"""
        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)
        self.training_episodes += 1


# ========================================================================
# 4ï¸âƒ£ Ø¯Ø§Ù„Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨
# ========================================================================

def train_lunar_lander(agent, env, n_episodes=2000, max_steps=1000, verbose=True):
    """
    ØªØ¯Ø±ÙŠØ¨ Ø§Ù„ÙˆÙƒÙŠÙ„ Ø¹Ù„Ù‰ ØªØ­Ø¯ÙŠ Lunar Lander
    
    Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª:
    -----------
    agent: ÙˆÙƒÙŠÙ„ Q-Learning
    env: Ø¨ÙŠØ¦Ø© Ø§Ù„ØªØ­Ø¯ÙŠ
    n_episodes: Ø¹Ø¯Ø¯ Ø§Ù„Ø­Ù„Ù‚Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ÙŠØ© (ÙŠÙÙ†ØµØ­ Ø¨Ù€ 2000+)
    max_steps: Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ù„Ø®Ø·ÙˆØ§Øª ÙÙŠ ÙƒÙ„ Ø­Ù„Ù‚Ø©
    verbose: Ø¹Ø±Ø¶ Ø§Ù„ØªÙ‚Ø¯Ù…
    
    Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª:
    ---------
    episode_rewards: Ù‚Ø§Ø¦Ù…Ø© Ø¨Ù…ÙƒØ§ÙØ¢Øª ÙƒÙ„ Ø­Ù„Ù‚Ø©
    episode_lengths: Ù‚Ø§Ø¦Ù…Ø© Ø¨Ø£Ø·ÙˆØ§Ù„ ÙƒÙ„ Ø­Ù„Ù‚Ø©
    success_count: Ø¹Ø¯Ø¯ Ù…Ø±Ø§Øª Ø§Ù„Ù‡Ø¨ÙˆØ· Ø§Ù„Ù†Ø§Ø¬Ø­
    """
    
    episode_rewards = []
    episode_lengths = []
    success_episodes = []
    
    print(" Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¹Ù„Ù‰ Lunar Lander...")
    print("=" * 70)
    print("  ØªÙ†Ø¨ÙŠÙ‡: Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù‚Ø¯ ÙŠØ³ØªØºØ±Ù‚ ÙˆÙ‚ØªØ§Ù‹ Ø·ÙˆÙŠÙ„Ø§Ù‹ (10-30 Ø¯Ù‚ÙŠÙ‚Ø©)")
    print("=" * 70)
    
    for episode in range(n_episodes):
        state = env.reset()
        episode_reward = 0
        
        for step in range(max_steps):
            # Ø§Ø®ØªÙŠØ§Ø± ÙˆØªÙ†ÙÙŠØ° Ø¥Ø¬Ø±Ø§Ø¡
            action = agent.get_action(state, training=True)
            next_state, reward, done, info = env.step(action)
            
            # ØªØ­Ø¯ÙŠØ« Q-table
            agent.update(state, action, reward, next_state, done)
            
            episode_reward += reward
            state = next_state
            
            if done:
                break
        
        # ØªÙ‚Ù„ÙŠÙ„ epsilon
        agent.decay_epsilon()
        
        episode_rewards.append(episode_reward)
        episode_lengths.append(step + 1)
        
        # ØªØªØ¨Ø¹ Ø§Ù„Ø­Ù„Ù‚Ø§Øª Ø§Ù„Ù†Ø§Ø¬Ø­Ø©
        if episode_reward >= 200:
            success_episodes.append(episode)
        
        # Ø¹Ø±Ø¶ Ø§Ù„ØªÙ‚Ø¯Ù…
        if verbose and (episode + 1) % 100 == 0:
            avg_reward = np.mean(episode_rewards[-100:])
            avg_length = np.mean(episode_lengths[-100:])
            success_rate = len([r for r in episode_rewards[-100:] if r >= 200])
            
            print(f"Ø§Ù„Ø­Ù„Ù‚Ø© {episode + 1:4d} | "
                  f"Ù…ØªÙˆØ³Ø· Ø§Ù„Ù…ÙƒØ§ÙØ£Ø©: {avg_reward:8.2f} | "
                  f"Ù…ØªÙˆØ³Ø· Ø§Ù„Ø·ÙˆÙ„: {avg_length:6.1f} | "
                  f"Ù†Ø¬Ø§Ø­Ø§Øª: {success_rate:2d}/100 | "
                  f"Epsilon: {agent.epsilon:.3f}")
    
    print("=" * 70)
    print("âœ… Ø§ÙƒØªÙ…Ù„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨!")
    print(f"   Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù‡Ø¨ÙˆØ· Ø§Ù„Ù†Ø§Ø¬Ø­: {len(success_episodes)}")
    
    return episode_rewards, episode_lengths, success_episodes


# ========================================================================
# 5ï¸âƒ£ Ø¯ÙˆØ§Ù„ Ø§Ù„ØªØµÙˆØ± ÙˆØ§Ù„ØªÙ‚ÙŠÙŠÙ…
# ========================================================================

def plot_training_results(episode_rewards, episode_lengths):
    """Ø±Ø³Ù… Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ¯Ø±ÙŠØ¨"""
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    fig.suptitle('ğŸ“Š Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ - ØªØ­Ø¯ÙŠ Lunar Lander', 
                 fontsize=16, weight='bold')
    
    # 1. Ù…Ù†Ø­Ù†Ù‰ Ø§Ù„Ù…ÙƒØ§ÙØ¢Øª
    ax1 = axes[0, 0]
    ax1.plot(episode_rewards, alpha=0.2, color='blue', label='Ø§Ù„Ù…ÙƒØ§ÙØ£Ø©')
    
    # Ø®Ø· Ø§Ù„Ù†Ø¬Ø§Ø­
    ax1.axhline(y=200, color='green', linestyle='--', 
                linewidth=2, label='Ø­Ø¯ Ø§Ù„Ù†Ø¬Ø§Ø­ (200)')
    
    window = 100
    if len(episode_rewards) >= window:
        moving_avg = np.convolve(episode_rewards, 
                                np.ones(window)/window, mode='valid')
        ax1.plot(range(window-1, len(episode_rewards)), 
                moving_avg, color='red', linewidth=2, 
                label=f'Ø§Ù„Ù…ØªÙˆØ³Ø· Ø§Ù„Ù…ØªØ­Ø±Ùƒ ({window})')
    
    ax1.set_xlabel('Ø±Ù‚Ù… Ø§Ù„Ø­Ù„Ù‚Ø©', fontsize=11)
    ax1.set_ylabel('Ø§Ù„Ù…ÙƒØ§ÙØ£Ø© Ø§Ù„ÙƒÙ„ÙŠØ©', fontsize=11)
    ax1.set_title('Ù…Ù†Ø­Ù†Ù‰ Ø§Ù„ØªØ¹Ù„Ù… - Ø§Ù„Ù…ÙƒØ§ÙØ¢Øª', fontsize=12, weight='bold')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # 2. Ø£Ø·ÙˆØ§Ù„ Ø§Ù„Ø­Ù„Ù‚Ø§Øª
    ax2 = axes[0, 1]
    ax2.plot(episode_lengths, alpha=0.2, color='green', label='Ø§Ù„Ø·ÙˆÙ„')
    
    if len(episode_lengths) >= window:
        moving_avg = np.convolve(episode_lengths, 
                                np.ones(window)/window, mode='valid')
        ax2.plot(range(window-1, len(episode_lengths)), 
                moving_avg, color='orange', linewidth=2, 
                label=f'Ø§Ù„Ù…ØªÙˆØ³Ø· Ø§Ù„Ù…ØªØ­Ø±Ùƒ ({window})')
    
    ax2.set_xlabel('Ø±Ù‚Ù… Ø§Ù„Ø­Ù„Ù‚Ø©', fontsize=11)
    ax2.set_ylabel('Ø¹Ø¯Ø¯ Ø§Ù„Ø®Ø·ÙˆØ§Øª', fontsize=11)
    ax2.set_title('Ø·ÙˆÙ„ Ø§Ù„Ø­Ù„Ù‚Ø§Øª', fontsize=12, weight='bold')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # 3. ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù…ÙƒØ§ÙØ¢Øª (Ø¢Ø®Ø± 200 Ø­Ù„Ù‚Ø©)
    ax3 = axes[1, 0]
    last_episodes = episode_rewards[-200:]
    ax3.hist(last_episodes, bins=40, color='purple', alpha=0.7, edgecolor='black')
    ax3.axvline(np.mean(last_episodes), color='red', linestyle='--', 
                linewidth=2, label=f'Ø§Ù„Ù…ØªÙˆØ³Ø·: {np.mean(last_episodes):.1f}')
    ax3.axvline(200, color='green', linestyle='--', 
                linewidth=2, label='Ø­Ø¯ Ø§Ù„Ù†Ø¬Ø§Ø­')
    ax3.set_xlabel('Ø§Ù„Ù…ÙƒØ§ÙØ£Ø©', fontsize=11)
    ax3.set_ylabel('Ø§Ù„ØªÙƒØ±Ø§Ø±', fontsize=11)
    ax3.set_title('ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù…ÙƒØ§ÙØ¢Øª (Ø¢Ø®Ø± 200 Ø­Ù„Ù‚Ø©)', fontsize=12, weight='bold')
    ax3.legend()
    ax3.grid(True, alpha=0.3, axis='y')
    
    # 4. Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù†Ø¬Ø§Ø­ Ø§Ù„Ù…ØªØ­Ø±Ùƒ
    ax4 = axes[1, 1]
    success_rates = []
    
    for i in range(100, len(episode_rewards), 10):
        recent = episode_rewards[i-100:i]
        success_rate = (np.array(recent) >= 200).mean() * 100
        success_rates.append(success_rate)
    
    ax4.plot(range(100, len(episode_rewards), 10), success_rates, 
            color='teal', linewidth=2, marker='o', markersize=3)
    ax4.axhline(y=90, color='gold', linestyle='--', 
                linewidth=2, alpha=0.5, label='Ù‡Ø¯Ù 90%')
    ax4.set_xlabel('Ø±Ù‚Ù… Ø§Ù„Ø­Ù„Ù‚Ø©', fontsize=11)
    ax4.set_ylabel('Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù†Ø¬Ø§Ø­ (%)', fontsize=11)
    ax4.set_title('Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù†Ø¬Ø§Ø­ (Ø¢Ø®Ø± 100 Ø­Ù„Ù‚Ø©)', fontsize=12, weight='bold')
    ax4.legend()
    ax4.grid(True, alpha=0.3)
    ax4.set_ylim(0, 105)
    
    plt.tight_layout()
    plt.show()


def evaluate_agent(agent, env, n_episodes=100):
    """
    ØªÙ‚ÙŠÙŠÙ… Ø§Ù„ÙˆÙƒÙŠÙ„ Ø§Ù„Ù…Ø¯Ø±Ø¨
    
    Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª:
    ---------
    dict: Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡
    """
    
    print("\n" + "=" * 70)
    print("ğŸ“ˆ ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ...")
    print("=" * 70)
    
    episode_rewards = []
    episode_lengths = []
    success_count = 0
    crash_count = 0
    
    for episode in range(n_episodes):
        state = env.reset()
        episode_reward = 0
        
        for step in range(1000):
            action = agent.get_action(state, training=False)
            next_state, reward, done, _ = env.step(action)
            
            episode_reward += reward
            state = next_state
            
            if done:
                if episode_reward >= 200:
                    success_count += 1
                elif episode_reward < -100:
                    crash_count += 1
                break
        
        episode_rewards.append(episode_reward)
        episode_lengths.append(step + 1)
    
    # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
    stats = {
        'Ù…ØªÙˆØ³Ø·_Ø§Ù„Ù…ÙƒØ§ÙØ£Ø©': np.mean(episode_rewards),
        'Ø§Ù†Ø­Ø±Ø§Ù_Ù…Ø¹ÙŠØ§Ø±ÙŠ_Ø§Ù„Ù…ÙƒØ§ÙØ£Ø©': np.std(episode_rewards),
        'Ø£ÙØ¶Ù„_Ù…ÙƒØ§ÙØ£Ø©': np.max(episode_rewards),
        'Ø£Ø³ÙˆØ£_Ù…ÙƒØ§ÙØ£Ø©': np.min(episode_rewards),
        'Ù…ØªÙˆØ³Ø·_Ø§Ù„Ø®Ø·ÙˆØ§Øª': np.mean(episode_lengths),
        'Ù…Ø¹Ø¯Ù„_Ø§Ù„Ù†Ø¬Ø§Ø­_%': (success_count / n_episodes) * 100,
        'Ù…Ø¹Ø¯Ù„_Ø§Ù„ØªØ­Ø·Ù…_%': (crash_count / n_episodes) * 100,
        'Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹_Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ': np.sum(episode_rewards)
    }
    
    # Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    print(f"\n Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¹Ù„Ù‰ {n_episodes} Ø­Ù„Ù‚Ø©:")
    print(f"   â€¢ Ù…ØªÙˆØ³Ø· Ø§Ù„Ù…ÙƒØ§ÙØ£Ø©: {stats['Ù…ØªÙˆØ³Ø·_Ø§Ù„Ù…ÙƒØ§ÙØ£Ø©']:.2f} Â± {stats['Ø§Ù†Ø­Ø±Ø§Ù_Ù…Ø¹ÙŠØ§Ø±ÙŠ_Ø§Ù„Ù…ÙƒØ§ÙØ£Ø©']:.2f}")
    print(f"   â€¢ Ø£ÙØ¶Ù„ Ù…ÙƒØ§ÙØ£Ø©: {stats['Ø£ÙØ¶Ù„_Ù…ÙƒØ§ÙØ£Ø©']:.2f}")
    print(f"   â€¢ Ø£Ø³ÙˆØ£ Ù…ÙƒØ§ÙØ£Ø©: {stats['Ø£Ø³ÙˆØ£_Ù…ÙƒØ§ÙØ£Ø©']:.2f}")
    print(f"   â€¢ Ù…ØªÙˆØ³Ø· Ø¹Ø¯Ø¯ Ø§Ù„Ø®Ø·ÙˆØ§Øª: {stats['Ù…ØªÙˆØ³Ø·_Ø§Ù„Ø®Ø·ÙˆØ§Øª']:.1f}")
    print(f"   â€¢ Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù‡Ø¨ÙˆØ· Ø§Ù„Ù†Ø§Ø¬Ø­: {stats['Ù…Ø¹Ø¯Ù„_Ø§Ù„Ù†Ø¬Ø§Ø­_%']:.1f}%")
    print(f"   â€¢ Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ­Ø·Ù…: {stats['Ù…Ø¹Ø¯Ù„_Ø§Ù„ØªØ­Ø·Ù…_%']:.1f}%")
    print(f"\n Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ù„Ù„Ù†Ù‚Ø§Ø·: {stats['Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹_Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ']:.0f}")
    
    # ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø£Ø¯Ø§Ø¡
    if stats['Ù…Ø¹Ø¯Ù„_Ø§Ù„Ù†Ø¬Ø§Ø­_%'] >= 90:
        print("â­â­â­ Ø£Ø¯Ø§Ø¡ Ù…Ù…ØªØ§Ø²! ÙˆØµÙ„Øª Ù„Ù„Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨!")
    elif stats['Ù…Ø¹Ø¯Ù„_Ø§Ù„Ù†Ø¬Ø§Ø­_%'] >= 70:
        print("â­â­ Ø£Ø¯Ø§Ø¡ Ø¬ÙŠØ¯ Ø¬Ø¯Ø§Ù‹! Ù‚Ø±ÙŠØ¨ Ù…Ù† Ø§Ù„Ù‡Ø¯Ù!")
    elif stats['Ù…Ø¹Ø¯Ù„_Ø§Ù„Ù†Ø¬Ø§Ø­_%'] >= 50:
        print("â­ Ø£Ø¯Ø§Ø¡ Ø¬ÙŠØ¯! ÙŠÙ…ÙƒÙ† ØªØ­Ø³ÙŠÙ†Ù‡ Ø£ÙƒØ«Ø±")
    else:
        print("ğŸ’¡ ÙŠØ­ØªØ§Ø¬ ØªØ­Ø³ÙŠÙ† - Ø¬Ø±Ø¨ ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø£Ùˆ Ø²ÙŠØ§Ø¯Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨")
    
    print("=" * 70)
    
    return stats


# ========================================================================
# 6ï¸âƒ£ Ø§Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
# ========================================================================

def main():
    """Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ Ù„Ù„ØªØ­Ø¯ÙŠ"""
    
    print("\n" + "=" * 70)
    print(" ØªØ­Ø¯ÙŠ Lunar Lander - Ø§Ù„Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø«Ø§Ù†ÙŠ (Ù…ØªÙˆØ³Ø·)")
    print("=" * 70)
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø¨ÙŠØ¦Ø©
    env = LunarLanderChallenge()
    
    # Ø¥Ø¹Ø¯Ø§Ø¯ ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø­Ø§Ù„Ø§Øª (ÙŠÙ…ÙƒÙ†Ùƒ ØªØ¹Ø¯ÙŠÙ„ Ù‡Ø°Ù‡ Ø§Ù„Ù‚ÙŠÙ…!)
    # ØªÙ†Ø¨ÙŠÙ‡: Ø²ÙŠØ§Ø¯Ø© bins_per_dimension ÙŠØ²ÙŠØ¯ Ø§Ù„Ø¯Ù‚Ø© Ù„ÙƒÙ† ÙŠØ¨Ø·Ø¦ Ø§Ù„ØªØ¯Ø±ÙŠØ¨
    env.setup_discretization(bins_per_dimension=10)
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ÙˆÙƒÙŠÙ„ (ÙŠÙ…ÙƒÙ†Ùƒ ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª!)
    agent = QLearningAgent(
        n_actions=4,
        learning_rate=0.1,         # Ø¬Ø±Ø¨: 0.05, 0.15, 0.2
        discount_factor=0.99,       # Ø¬Ø±Ø¨: 0.95, 0.98, 0.999
        epsilon_start=1.0,
        epsilon_end=0.01,
        epsilon_decay=0.9995        # Ø¬Ø±Ø¨: 0.995, 0.999
    )
    
    print("\nâš™ï¸  Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©:")
    print(f"   â€¢ Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù… (Î±): {agent.learning_rate}")
    print(f"   â€¢ Ù…Ø¹Ø§Ù…Ù„ Ø§Ù„Ø®ØµÙ… (Î³): {agent.discount_factor}")
    print(f"   â€¢ Epsilon Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ: {agent.epsilon_end}")
    print(f"   â€¢ Ù…Ø¹Ø¯Ù„ ØªÙ†Ø§Ù‚Øµ Epsilon: {agent.epsilon_decay}")
    print(f"   â€¢ ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø­Ø§Ù„Ø§Øª: 10^8 (8 Ø£Ø¨Ø¹Ø§Ø¯)")
    
    # Ø§Ù„ØªØ¯Ø±ÙŠØ¨
    print("\n Ù†ØµÙŠØ­Ø©: Lunar Lander Ø£ØµØ¹Ø¨ Ù…Ù† Mountain Car")
    print("   Ù‚Ø¯ ØªØ­ØªØ§Ø¬ 2000+ Ø­Ù„Ù‚Ø© Ù„Ù„ÙˆØµÙˆÙ„ Ù„Ø£Ø¯Ø§Ø¡ Ø¬ÙŠØ¯")
    
    episode_rewards, episode_lengths, success_episodes = train_lunar_lander(
        agent, env, 
        n_episodes=2000,  # ÙŠÙ…ÙƒÙ†Ùƒ Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø¹Ø¯Ø¯ Ù„Ù†ØªØ§Ø¦Ø¬ Ø£ÙØ¶Ù„
        max_steps=1000,
        verbose=True
    )
    
    # Ø±Ø³Ù… Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    plot_training_results(episode_rewards, episode_lengths)
    
    # Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
    final_stats = evaluate_agent(agent, env, n_episodes=100)
    
    # Ø¥ØºÙ„Ø§Ù‚ Ø§Ù„Ø¨ÙŠØ¦Ø©
    env.close()
    
    return agent, env, final_stats


# ========================================================================
#  ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ­Ø¯ÙŠ
# ========================================================================

if __name__ == "__main__":
    agent, env, stats = main()
    
    print("\n Ø§Ù†ØªÙ‡Ù‰ Ø§Ù„ØªØ­Ø¯ÙŠ!")
    print(" Ù†ØµØ§Ø¦Ø­ Ù„Ù„ØªØ­Ø³ÙŠÙ†:")
    print("   - Ø¬Ø±Ø¨ Ø²ÙŠØ§Ø¯Ø© Ø¹Ø¯Ø¯ Ø­Ù„Ù‚Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ (3000-5000)")
    print("   - Ø§Ø¶Ø¨Ø· Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù… (learning_rate)")
    print("   - Ø¬Ø±Ø¨ Ù‚ÙŠÙ… Ù…Ø®ØªÙ„ÙØ© Ù„Ù€ epsilon_decay")
    print("   - ÙŠÙ…ÙƒÙ†Ùƒ ØªÙ‚Ù„ÙŠÙ„ bins_per_dimension Ù„Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø£Ø³Ø±Ø¹")
    print("\n Ù„Ø§ ØªÙ†Ø³Ù ØªÙˆØ«ÙŠÙ‚ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØªÙƒ ÙˆØ§Ù„ØªØºÙŠÙŠØ±Ø§Øª!")
