#!/usr/bin/env python3
# -*- coding: utf-8 -*-

# ========================================================================
# ØªØ­Ø¯ÙŠ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø°Ø§ØªÙŠ (Ù„Ù„Ù…ØªØ®ØµØµÙŠÙ†)
# Ø§Ù„Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø«Ø§Ù„Ø«: Ø³Ø¨Ø§Ù‚ ØªÙˆÙÙŠØ± Ø§Ù„ÙˆÙ‚ÙˆØ¯ (Eco-Racing)
# Ø§Ù„ØµØ¹ÙˆØ¨Ø©: ØµØ¹Ø¨
# 
# Ø§Ø³Ù… Ø§Ù„Ù…Ù„Ù: eco_racing_challenge.py
# ØªØ§Ø±ÙŠØ® Ø§Ù„Ø¥Ù†Ø´Ø§Ø¡: 2025
# ========================================================================

"""
 ÙˆØµÙ Ø§Ù„ØªØ­Ø¯ÙŠ:
--------------
Ø³ÙŠØ§Ø±Ø© Ø³Ø¨Ø§Ù‚ ÙŠØ¬Ø¨ Ø¹Ù„ÙŠÙ‡Ø§ Ø¥ÙƒÙ…Ø§Ù„ Ù…Ø³Ø§Ø± Ø§Ù„Ø³Ø¨Ø§Ù‚ Ø¨Ø£Ø³Ø±Ø¹ ÙˆÙ‚Øª Ù…Ù…ÙƒÙ†
Ù…Ø¹ ØªÙˆÙÙŠØ± Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ Ø§Ù„ÙˆÙ‚ÙˆØ¯. Ø§Ù„ØªØ­Ø¯ÙŠ ÙŠØ¬Ù…Ø¹ Ø¨ÙŠÙ† Ø§Ù„Ø³Ø±Ø¹Ø© ÙˆØ§Ù„ÙƒÙØ§Ø¡Ø©.

Ø§Ù„Ø¨ÙŠØ¦Ø©: CarRacing-v2 Ù…Ù† Gymnasium
Ø§Ù„Ù‡Ø¯Ù: Ù‚ÙŠØ§Ø¯Ø© Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ø£Ø®Ø¶Ø± ÙˆØªØ¬Ù†Ø¨ Ø§Ù„Ø®Ø±ÙˆØ¬ Ø¹Ù†Ù‡

 Ø§Ù„Ù‚ÙˆØ§Ù†ÙŠÙ† ÙˆØ§Ù„Ù‚ÙŠÙˆØ¯:
-------------------
1. ÙŠØ¬Ø¨ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© Q-Learning ÙÙ‚Ø·
2. Ù„Ø§ ÙŠØ³Ù…Ø­ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Deep Learning Ø£Ùˆ Neural Networks
3. Ø§Ù„ØµÙˆØ±Ø© 96Ã—96Ã—3 ÙŠØ¬Ø¨ ØªØ¨Ø³ÙŠØ·Ù‡Ø§ Ù„Ø­Ø§Ù„Ø§Øª Ù…Ù†ÙØµÙ„Ø©
4. Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ù…Ø³Ù…ÙˆØ­Ø© (Ù…Ø³ØªÙ…Ø±Ø©):
   - Ø§Ù„ØªÙˆØ¬ÙŠÙ‡: -1 (ÙŠØ³Ø§Ø±) Ø¥Ù„Ù‰ +1 (ÙŠÙ…ÙŠÙ†)
   - Ø§Ù„ØªØ³Ø§Ø±Ø¹: 0 Ø¥Ù„Ù‰ +1
   - Ø§Ù„ÙØ±Ø§Ù…Ù„: 0 Ø¥Ù„Ù‰ +1
5. Ø§Ù„Ù†Ø¬Ø§Ø­ = Ø¥ÙƒÙ…Ø§Ù„ Ø§Ù„Ù…Ø³Ø§Ø± Ø¨Ù…ÙƒØ§ÙØ£Ø© Ø¹Ø§Ù„ÙŠØ©

 Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„ØªÙ‚ÙŠÙŠÙ…:
------------------
- Ø§Ù„Ø¨Ù‚Ø§Ø¡ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø³Ø§Ø±: +1000/Ø¯ÙˆØ±Ø© ÙƒØ§Ù…Ù„Ø©
- Ø§Ù„Ø®Ø±ÙˆØ¬ Ø¹Ù† Ø§Ù„Ù…Ø³Ø§Ø±: -0.1 Ù„ÙƒÙ„ Ø¥Ø·Ø§Ø±
- Ø§Ù„Ø³Ø±Ø¹Ø© Ø§Ù„Ø¹Ø§Ù„ÙŠØ©: Ù…ÙƒØ§ÙØ¢Øª Ø¥Ø¶Ø§ÙÙŠØ©
- Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ: Ù…ØªÙˆØ³Ø· Ø¢Ø®Ø± 100 Ø­Ù„Ù‚Ø©

 ØªÙ†Ø¨ÙŠÙ‡Ø§Øª Ù‡Ø§Ù…Ø©:
-----------------
- Ù‡Ø°Ø§ Ø§Ù„ØªØ­Ø¯ÙŠ ØµØ¹Ø¨ Ø¬Ø¯Ø§Ù‹ Ù…Ø¹ Q-Learning Ø§Ù„ØªÙ‚Ù„ÙŠØ¯ÙŠ
- ÙŠØ­ØªØ§Ø¬ Ø¥Ø¨Ø¯Ø§Ø¹ ÙÙŠ ØªØ¨Ø³ÙŠØ· Ø§Ù„Ù…Ø´Ø§Ù‡Ø¯Ø§Øª Ø§Ù„Ø¨ØµØ±ÙŠØ©
- Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù‚Ø¯ ÙŠØ³ØªØºØ±Ù‚ ÙˆÙ‚ØªØ§Ù‹ Ø·ÙˆÙŠÙ„Ø§Ù‹ Ø¬Ø¯Ø§Ù‹
- ÙŠÙÙ†ØµØ­ Ø¨ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø¥Ù„Ù‰ Ø®ÙŠØ§Ø±Ø§Øª Ù…Ù†ÙØµÙ„Ø©
"""

# ========================================================================
# 1ï¸âƒ£ Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
# ========================================================================

import numpy as np
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation
from IPython.display import HTML
import gymnasium as gym
from collections import defaultdict
import cv2
import warnings
warnings.filterwarnings('ignore')

# ========================================================================
# 2ï¸âƒ£ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¨ÙŠØ¦Ø© (Ù„Ø§ ØªØ¹Ø¯Ù„ Ù‡Ø°Ø§ Ø§Ù„Ù‚Ø³Ù…!)
# ========================================================================

class EcoRacingChallenge:
    """
    Ø¨ÙŠØ¦Ø© ØªØ­Ø¯ÙŠ Eco-Racing
     Ù…Ù…Ù†ÙˆØ¹ Ø§Ù„ØªØ¹Ø¯ÙŠÙ„ Ø¹Ù„Ù‰ Ù‡Ø°Ø§ Ø§Ù„ÙƒÙ„Ø§Ø³!
    
    Ø§Ù„ØªØ­Ø¯ÙŠ:
    - ØªØ­ÙˆÙŠÙ„ ØµÙˆØ±Ø© 96Ã—96Ã—3 Ø¥Ù„Ù‰ Ø­Ø§Ù„Ø© Ù…Ù†ÙØµÙ„Ø©
    - ØªØ¨Ø³ÙŠØ· Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ù…Ø³ØªÙ…Ø±Ø© Ø¥Ù„Ù‰ Ù…Ù†ÙØµÙ„Ø©
    - Ø§Ù„Ù…ÙˆØ§Ø²Ù†Ø© Ø¨ÙŠÙ† Ø§Ù„Ø³Ø±Ø¹Ø© ÙˆØªÙˆÙÙŠØ± Ø§Ù„ÙˆÙ‚ÙˆØ¯
    """
    
    def __init__(self):
        self.env = gym.make('CarRacing-v2', continuous=False)
        self.action_space_size = 5  # Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ù…Ù†ÙØµÙ„Ø© Ù…Ø¨Ø³Ø·Ø©
        
    def simplify_observation(self, observation):
        """
        ØªØ¨Ø³ÙŠØ· Ø§Ù„Ù…Ø´Ø§Ù‡Ø¯Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ© Ø¥Ù„Ù‰ Ø­Ø§Ù„Ø© Ù…Ù†ÙØµÙ„Ø©
        
        Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ©:
        - ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø© Ù„Ù€ grayscale
        - ØªÙ‚Ø³ÙŠÙ… Ø§Ù„ØµÙˆØ±Ø© Ø¥Ù„Ù‰ Ø´Ø¨ÙƒØ© (grid)
        - Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ø£Ø®Ø¶Ø± ÙÙŠ ÙƒÙ„ Ø®Ù„ÙŠØ©
        - Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ø³ÙŠØ§Ø±Ø© ÙˆØ§ØªØ¬Ø§Ù‡Ù‡Ø§
        """
        # ØªØ­ÙˆÙŠÙ„ Ù„Ù€ grayscale
        gray = cv2.cvtColor(observation, cv2.COLOR_RGB2GRAY)
        
        # ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø­Ø¬Ù…
        small = cv2.resize(gray, (12, 12))
        
        # ØªÙ‚Ø³ÙŠÙ… Ø¥Ù„Ù‰ 4 Ù…Ø³ØªÙˆÙŠØ§Øª
        discretized = (small / 64).astype(int)
        discretized = np.clip(discretized, 0, 3)
        
        # Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ù…Ø³Ø§Ø± (Ø§Ù„Ù„ÙˆÙ† Ø§Ù„Ø£Ø®Ø¶Ø±)
        green_channel = observation[:, :, 1]
        track_indicator = (green_channel > 100).astype(int)
        track_sum = track_indicator.sum()
        
        # ØªØ¨Ø³ÙŠØ· Ø¥Ù„Ù‰ 5 Ù…Ø³ØªÙˆÙŠØ§Øª
        on_track = min(4, track_sum // 1000)
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø§ØªØ¬Ø§Ù‡ (ØªÙ‚Ø±ÙŠØ¨ÙŠ Ù…Ù† Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø³ÙÙ„ÙŠ Ù…Ù† Ø§Ù„ØµÙˆØ±Ø©)
        bottom_section = observation[60:80, :, :]
        left_green = bottom_section[:, :32, 1].mean()
        center_green = bottom_section[:, 32:64, 1].mean()
        right_green = bottom_section[:, 64:, 1].mean()
        
        if center_green > max(left_green, right_green):
            direction = 1  # ÙÙŠ Ø§Ù„Ù…Ù†ØªØµÙ
        elif left_green > right_green:
            direction = 0  # ÙŠØ³Ø§Ø±
        else:
            direction = 2  # ÙŠÙ…ÙŠÙ†
        
        return (on_track, direction, tuple(discretized.flatten()[:20]))
    
    def reset(self):
        """Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ† Ø§Ù„Ø¨ÙŠØ¦Ø©"""
        observation, _ = self.env.reset()
        return self.simplify_observation(observation)
    
    def step(self, action):
        """
        ØªÙ†ÙÙŠØ° Ø®Ø·ÙˆØ© ÙÙŠ Ø§Ù„Ø¨ÙŠØ¦Ø©
        
        ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ù…Ù†ÙØµÙ„Ø© Ø¥Ù„Ù‰ Ù…Ø³ØªÙ…Ø±Ø©:
        0: Ù„Ø§ Ø´ÙŠØ¡
        1: ÙŠØ³Ø§Ø±
        2: ÙŠÙ…ÙŠÙ†
        3: ØªØ³Ø§Ø±Ø¹
        4: ÙØ±Ø§Ù…Ù„
        
        Ø§Ù„Ù…ÙƒØ§ÙØ¢Øª (Ù„Ø§ ÙŠÙ…ÙƒÙ† ØªØ¹Ø¯ÙŠÙ„Ù‡Ø§):
        - Ø§Ù„Ø¨Ù‚Ø§Ø¡ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø³Ø§Ø±: Ø¥ÙŠØ¬Ø§Ø¨ÙŠ
        - Ø§Ù„Ø®Ø±ÙˆØ¬ Ø¹Ù† Ø§Ù„Ù…Ø³Ø§Ø±: Ø³Ù„Ø¨ÙŠ
        - Ø§Ù„Ø³Ø±Ø¹Ø©: Ù…ÙƒØ§ÙØ¢Øª Ø¥Ø¶Ø§ÙÙŠØ©
        """
        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡ Ø§Ù„Ù…Ù†ÙØµÙ„ Ø¥Ù„Ù‰ Ù…Ø³ØªÙ…Ø±
        action_map = {
            0: [0, 0, 0],      # Ù„Ø§ Ø´ÙŠØ¡
            1: [-1, 0, 0],     # ÙŠØ³Ø§Ø±
            2: [1, 0, 0],      # ÙŠÙ…ÙŠÙ†
            3: [0, 1, 0],      # ØªØ³Ø§Ø±Ø¹
            4: [0, 0, 0.8],    # ÙØ±Ø§Ù…Ù„
        }
        
        continuous_action = action_map.get(action, [0, 0, 0])
        
        observation, reward, terminated, truncated, info = self.env.step(
            continuous_action
        )
        done = terminated or truncated
        
        return self.simplify_observation(observation), reward, done, info
    
    def render(self):
        """Ø¹Ø±Ø¶ Ø§Ù„Ø¨ÙŠØ¦Ø©"""
        return self.env.render()
    
    def close(self):
        """Ø¥ØºÙ„Ø§Ù‚ Ø§Ù„Ø¨ÙŠØ¦Ø©"""
        self.env.close()


# ========================================================================
# 3ï¸âƒ£ Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© Q-Learning (ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„ØªØ¹Ø¯ÙŠÙ„ Ù‡Ù†Ø§!)
# ========================================================================

class QLearningAgent:
    """
    ÙˆÙƒÙŠÙ„ Q-Learning Ù„ØªØ­Ø¯ÙŠ Eco-Racing
    
     ÙŠÙ…ÙƒÙ†Ùƒ ØªØ¹Ø¯ÙŠÙ„:
    - Ù‚ÙŠÙ… Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª
    - Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„Ø§Ø³ØªÙƒØ´Ø§Ù
    - Ø·Ø±ÙŠÙ‚Ø© Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„Ù…Ø¹Ù‚Ø¯Ø©
    
     Ù„Ø§ ÙŠÙ…ÙƒÙ†Ùƒ:
    - Ø§Ø³ØªØ®Ø¯Ø§Ù… Neural Networks
    - ØªØºÙŠÙŠØ± Ø§Ù„Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
    
  
    """
    
    def __init__(self, 
                 n_actions=5,
                 learning_rate=0.1,
                 discount_factor=0.99,
                 epsilon_start=1.0,
                 epsilon_end=0.05,
                 epsilon_decay=0.9995):
        
        self.n_actions = n_actions
        self.learning_rate = learning_rate
        self.discount_factor = discount_factor
        self.epsilon = epsilon_start
        self.epsilon_end = epsilon_end
        self.epsilon_decay = epsilon_decay
        
        # Ø¬Ø¯ÙˆÙ„ Q Ù…Ø¹ Ù‚ÙŠÙ… Ø§ÙØªØ±Ø§Ø¶ÙŠØ©
        self.q_table = defaultdict(lambda: np.zeros(n_actions))
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
        self.training_episodes = 0
    
    def get_action(self, state, training=True):
        """Ø§Ø®ØªÙŠØ§Ø± Ø¥Ø¬Ø±Ø§Ø¡ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… epsilon-greedy"""
        if training and np.random.random() < self.epsilon:
            return np.random.randint(0, self.n_actions)
        else:
            q_values = self.q_table[state]
            max_q = np.max(q_values)
            best_actions = np.where(q_values == max_q)[0]
            return np.random.choice(best_actions)
    
    def update(self, state, action, reward, next_state, done):
        """ØªØ­Ø¯ÙŠØ« Ø¬Ø¯ÙˆÙ„ Q"""
        current_q = self.q_table[state][action]
        
        if done:
            max_next_q = 0
        else:
            max_next_q = np.max(self.q_table[next_state])
        
        target_q = reward + self.discount_factor * max_next_q
        new_q = current_q + self.learning_rate * (target_q - current_q)
        self.q_table[state][action] = new_q
    
    def decay_epsilon(self):
        """ØªÙ‚Ù„ÙŠÙ„ epsilon"""
        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)
        self.training_episodes += 1


# ========================================================================
# 4ï¸âƒ£ Ø¯Ø§Ù„Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨
# ========================================================================

def train_eco_racing(agent, env, n_episodes=1000, max_steps=1000, verbose=True):
    """
    ØªØ¯Ø±ÙŠØ¨ Ø§Ù„ÙˆÙƒÙŠÙ„ Ø¹Ù„Ù‰ ØªØ­Ø¯ÙŠ Eco-Racing
    
    
    """
    
    episode_rewards = []
    episode_lengths = []
    
    print("  Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¹Ù„Ù‰ Eco-Racing...")
    print("=" * 70)
    print("   Q-Learning Ø§Ù„ØªÙ‚Ù„ÙŠØ¯ÙŠ Ù‚Ø¯ Ù„Ø§ ÙŠÙƒÙˆÙ† ÙƒØ§ÙÙŠØ§Ù‹")
    print("   Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù‚Ø¯ ÙŠØ³ØªØºØ±Ù‚ ÙˆÙ‚ØªØ§Ù‹ Ø·ÙˆÙŠÙ„Ø§Ù‹ Ø¬Ø¯Ø§Ù‹ (Ø³Ø§Ø¹Ø§Øª)")
    print("=" * 70)
    
    for episode in range(n_episodes):
        state = env.reset()
        episode_reward = 0
        
        for step in range(max_steps):
            action = agent.get_action(state, training=True)
            next_state, reward, done, info = env.step(action)
            
            agent.update(state, action, reward, next_state, done)
            
            episode_reward += reward
            state = next_state
            
            if done:
                break
        
        agent.decay_epsilon()
        
        episode_rewards.append(episode_reward)
        episode_lengths.append(step + 1)
        
        if verbose and (episode + 1) % 50 == 0:
            avg_reward = np.mean(episode_rewards[-50:])
            avg_length = np.mean(episode_lengths[-50:])
            best_reward = max(episode_rewards[-50:])
            
            print(f"Ø§Ù„Ø­Ù„Ù‚Ø© {episode + 1:4d} | "
                  f"Ù…ØªÙˆØ³Ø· Ø§Ù„Ù…ÙƒØ§ÙØ£Ø©: {avg_reward:8.2f} | "
                  f"Ø£ÙØ¶Ù„: {best_reward:8.2f} | "
                  f"Ø§Ù„Ø·ÙˆÙ„: {avg_length:6.1f} | "
                  f"Epsilon: {agent.epsilon:.3f}")
    
    print("=" * 70)
    print(" Ø§ÙƒØªÙ…Ù„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨!")
    
    return episode_rewards, episode_lengths


# ========================================================================
# 5ï¸âƒ£ Ø¯ÙˆØ§Ù„ Ø§Ù„ØªØµÙˆØ± ÙˆØ§Ù„ØªÙ‚ÙŠÙŠÙ…
# ========================================================================

def plot_training_results(episode_rewards, episode_lengths):
    """Ø±Ø³Ù… Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ¯Ø±ÙŠØ¨"""
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    fig.suptitle(' Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ - ØªØ­Ø¯ÙŠ Eco-Racing', 
                 fontsize=16, weight='bold')
    
    # Ù…Ù†Ø­Ù†Ù‰ Ø§Ù„Ù…ÙƒØ§ÙØ¢Øª
    ax1 = axes[0, 0]
    ax1.plot(episode_rewards, alpha=0.3, color='blue')
    
    window = 50
    if len(episode_rewards) >= window:
        moving_avg = np.convolve(episode_rewards, 
                                np.ones(window)/window, mode='valid')
        ax1.plot(range(window-1, len(episode_rewards)), 
                moving_avg, color='red', linewidth=2)
    
    ax1.set_xlabel('Ø±Ù‚Ù… Ø§Ù„Ø­Ù„Ù‚Ø©')
    ax1.set_ylabel('Ø§Ù„Ù…ÙƒØ§ÙØ£Ø© Ø§Ù„ÙƒÙ„ÙŠØ©')
    ax1.set_title('Ù…Ù†Ø­Ù†Ù‰ Ø§Ù„ØªØ¹Ù„Ù… - Ø§Ù„Ù…ÙƒØ§ÙØ¢Øª')
    ax1.grid(True, alpha=0.3)
    
    # Ø£Ø·ÙˆØ§Ù„ Ø§Ù„Ø­Ù„Ù‚Ø§Øª
    ax2 = axes[0, 1]
    ax2.plot(episode_lengths, alpha=0.3, color='green')
    
    if len(episode_lengths) >= window:
        moving_avg = np.convolve(episode_lengths, 
                                np.ones(window)/window, mode='valid')
        ax2.plot(range(window-1, len(episode_lengths)), 
                moving_avg, color='orange', linewidth=2)
    
    ax2.set_xlabel('Ø±Ù‚Ù… Ø§Ù„Ø­Ù„Ù‚Ø©')
    ax2.set_ylabel('Ø¹Ø¯Ø¯ Ø§Ù„Ø®Ø·ÙˆØ§Øª')
    ax2.set_title('Ø·ÙˆÙ„ Ø§Ù„Ø­Ù„Ù‚Ø§Øª')
    ax2.grid(True, alpha=0.3)
    
    # ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù…ÙƒØ§ÙØ¢Øª
    ax3 = axes[1, 0]
    last_100 = episode_rewards[-100:]
    ax3.hist(last_100, bins=30, color='purple', alpha=0.7, edgecolor='black')
    ax3.axvline(np.mean(last_100), color='red', linestyle='--', linewidth=2)
    ax3.set_xlabel('Ø§Ù„Ù…ÙƒØ§ÙØ£Ø©')
    ax3.set_ylabel('Ø§Ù„ØªÙƒØ±Ø§Ø±')
    ax3.set_title('ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù…ÙƒØ§ÙØ¢Øª (Ø¢Ø®Ø± 100 Ø­Ù„Ù‚Ø©)')
    ax3.grid(True, alpha=0.3, axis='y')
    
    # Ø£ÙØ¶Ù„ Ø§Ù„Ù…ÙƒØ§ÙØ¢Øª
    ax4 = axes[1, 1]
    best_rewards = []
    for i in range(50, len(episode_rewards), 10):
        best_rewards.append(max(episode_rewards[i-50:i]))
    
    ax4.plot(range(50, len(episode_rewards), 10), best_rewards, 
            color='gold', linewidth=2, marker='o', markersize=3)
    ax4.set_xlabel('Ø±Ù‚Ù… Ø§Ù„Ø­Ù„Ù‚Ø©')
    ax4.set_ylabel('Ø£ÙØ¶Ù„ Ù…ÙƒØ§ÙØ£Ø©')
    ax4.set_title('Ø£ÙØ¶Ù„ Ø£Ø¯Ø§Ø¡ (Ø¢Ø®Ø± 50 Ø­Ù„Ù‚Ø©)')
    ax4.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()


def evaluate_agent(agent, env, n_episodes=20):
    """ØªÙ‚ÙŠÙŠÙ… Ø§Ù„ÙˆÙƒÙŠÙ„ Ø§Ù„Ù…Ø¯Ø±Ø¨"""
    
    print("\n" + "=" * 70)
    print(" ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ...")
    print("=" * 70)
    
    episode_rewards = []
    episode_lengths = []
    
    for episode in range(n_episodes):
        state = env.reset()
        episode_reward = 0
        
        for step in range(1000):
            action = agent.get_action(state, training=False)
            next_state, reward, done, _ = env.step(action)
            
            episode_reward += reward
            state = next_state
            
            if done:
                break
        
        episode_rewards.append(episode_reward)
        episode_lengths.append(step + 1)
    
    stats = {
        'Ù…ØªÙˆØ³Ø·_Ø§Ù„Ù…ÙƒØ§ÙØ£Ø©': np.mean(episode_rewards),
        'Ø§Ù†Ø­Ø±Ø§Ù_Ù…Ø¹ÙŠØ§Ø±ÙŠ_Ø§Ù„Ù…ÙƒØ§ÙØ£Ø©': np.std(episode_rewards),
        'Ø£ÙØ¶Ù„_Ù…ÙƒØ§ÙØ£Ø©': np.max(episode_rewards),
        'Ø£Ø³ÙˆØ£_Ù…ÙƒØ§ÙØ£Ø©': np.min(episode_rewards),
        'Ù…ØªÙˆØ³Ø·_Ø§Ù„Ø®Ø·ÙˆØ§Øª': np.mean(episode_lengths),
        'Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹_Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ': np.sum(episode_rewards)
    }
    
    print(f"\n Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¹Ù„Ù‰ {n_episodes} Ø­Ù„Ù‚Ø©:")
    print(f"   â€¢ Ù…ØªÙˆØ³Ø· Ø§Ù„Ù…ÙƒØ§ÙØ£Ø©: {stats['Ù…ØªÙˆØ³Ø·_Ø§Ù„Ù…ÙƒØ§ÙØ£Ø©']:.2f} Â± {stats['Ø§Ù†Ø­Ø±Ø§Ù_Ù…Ø¹ÙŠØ§Ø±ÙŠ_Ø§Ù„Ù…ÙƒØ§ÙØ£Ø©']:.2f}")
    print(f"   â€¢ Ø£ÙØ¶Ù„ Ù…ÙƒØ§ÙØ£Ø©: {stats['Ø£ÙØ¶Ù„_Ù…ÙƒØ§ÙØ£Ø©']:.2f}")
    print(f"   â€¢ Ø£Ø³ÙˆØ£ Ù…ÙƒØ§ÙØ£Ø©: {stats['Ø£Ø³ÙˆØ£_Ù…ÙƒØ§ÙØ£Ø©']:.2f}")
    print(f"   â€¢ Ù…ØªÙˆØ³Ø· Ø¹Ø¯Ø¯ Ø§Ù„Ø®Ø·ÙˆØ§Øª: {stats['Ù…ØªÙˆØ³Ø·_Ø§Ù„Ø®Ø·ÙˆØ§Øª']:.1f}")
    print(f"\n Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ù„Ù„Ù†Ù‚Ø§Ø·: {stats['Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹_Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ']:.0f}")
    
    print("\n Ù…Ù„Ø§Ø­Ø¸Ø©:")
    print("   Ù‡Ø°Ø§ Ø§Ù„ØªØ­Ø¯ÙŠ ØµØ¹Ø¨ Ø¬Ø¯Ø§Ù‹ Ù…Ø¹ Q-Learning Ø§Ù„ØªÙ‚Ù„ÙŠØ¯ÙŠ")
    print("   Ø§Ù„Ù…ÙƒØ§ÙØ¢Øª Ø§Ù„Ø³Ù„Ø¨ÙŠØ© Ù…ØªÙˆÙ‚Ø¹Ø© - Ø§Ù„Ù‡Ø¯Ù Ù‡Ùˆ Ø§Ù„ØªØ­Ø³Ù† Ø§Ù„ØªØ¯Ø±ÙŠØ¬ÙŠ")
    
    print("=" * 70)
    
    return stats


# ========================================================================
# 6ï¸âƒ£ Ø§Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
# ========================================================================

def main():
    """Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ Ù„Ù„ØªØ­Ø¯ÙŠ"""
    
    print("\n" + "=" * 70)
    print("ğŸï¸  ØªØ­Ø¯ÙŠ Eco-Racing - Ø§Ù„Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø«Ø§Ù„Ø« (ØµØ¹Ø¨)")
    print("=" * 70)
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø¨ÙŠØ¦Ø©
    env = EcoRacingChallenge()
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ÙˆÙƒÙŠÙ„
    agent = QLearningAgent(
        n_actions=5,
        learning_rate=0.2,
        discount_factor=0.95,
        epsilon_start=1.0,
        epsilon_end=0.05,
        epsilon_decay=0.9995
    )
    
    print("\nâš™ï¸  Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©:")
    print(f"   â€¢ Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù… (Î±): {agent.learning_rate}")
    print(f"   â€¢ Ù…Ø¹Ø§Ù…Ù„ Ø§Ù„Ø®ØµÙ… (Î³): {agent.discount_factor}")
    print(f"   â€¢ Epsilon Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ: {agent.epsilon_end}")
    print(f"   â€¢ Ù…Ø¹Ø¯Ù„ ØªÙ†Ø§Ù‚Øµ Epsilon: {agent.epsilon_decay}")
    
    print("\n  ØªØ­Ø°ÙŠØ± Ù…Ù‡Ù…:")
    print("   Ù‡Ø°Ø§ Ø§Ù„ØªØ­Ø¯ÙŠ Ù…Ø¹Ù‚Ø¯ Ø¬Ø¯Ø§Ù‹ Ù„Ù€ Q-Learning Ø§Ù„ØªÙ‚Ù„ÙŠØ¯ÙŠ")
    print("   Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ù‚Ø¯ ØªÙƒÙˆÙ† Ù…Ø­Ø¯ÙˆØ¯Ø© Ù…Ù‚Ø§Ø±Ù†Ø© Ø¨Ù€ Deep RL")
    print("   Ø§Ù„Ù‡Ø¯Ù: Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„ÙØ´Ù„ ÙˆØ§Ù„ØªØ­Ø³Ù† Ø§Ù„ØªØ¯Ø±ÙŠØ¬ÙŠ")
    
    # Ø§Ù„ØªØ¯Ø±ÙŠØ¨
    episode_rewards, episode_lengths = train_eco_racing(
        agent, env, 
        n_episodes=500,
        max_steps=1000,
        verbose=True
    )
    
    # Ø±Ø³Ù… Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    plot_training_results(episode_rewards, episode_lengths)
    
    # Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
    final_stats = evaluate_agent(agent, env, n_episodes=20)
    
    # Ø¥ØºÙ„Ø§Ù‚ Ø§Ù„Ø¨ÙŠØ¦Ø©
    env.close()
    
    return agent, env, final_stats


# ========================================================================
#  ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ­Ø¯ÙŠ
# ========================================================================

if __name__ == "__main__":
    print("\n" + "=" * 70)
    print("  ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©:")
    print("   pip install gymnasium opencv-python")
    print("   pip install gymnasium[box2d]")
    print("=" * 70)
    
    agent, env, stats = main()
    
    print("\n Ø§Ù†ØªÙ‡Ù‰ Ø§Ù„ØªØ­Ø¯ÙŠ!")
    print("\n Ù†ØµØ§Ø¦Ø­:")
    print("   - Ù‡Ø°Ø§ Ø§Ù„ØªØ­Ø¯ÙŠ ÙŠØ­ØªØ§Ø¬ Deep RL Ù„Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø¬ÙŠØ¯")
    print("   - Q-Learning Ø§Ù„ØªÙ‚Ù„ÙŠØ¯ÙŠ Ù…Ø­Ø¯ÙˆØ¯ Ù‡Ù†Ø§")
    print("   - Ø§Ù„Ù‡Ø¯Ù: ÙÙ‡Ù… Ø­Ø¯ÙˆØ¯ Ø§Ù„Ø·Ø±Ù‚ Ø§Ù„ØªÙ‚Ù„ÙŠØ¯ÙŠØ©")
    print("   - Ø¬Ø±Ø¨ ØªØ­Ø³ÙŠÙ† ØªØ¨Ø³ÙŠØ· Ø§Ù„Ø­Ø§Ù„Ø§Øª")
