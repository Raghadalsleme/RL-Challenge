#!/usr/bin/env python3
# -*- coding: utf-8 -*-

# ========================================================================
# ØªØ­Ø¯ÙŠ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø°Ø§ØªÙŠ (Ù„Ù„Ù…ØªØ®ØµØµÙŠÙ†)
# Ø§Ù„Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø®Ø§Ù…Ø³: Ø§Ù„ØªÙˆØ§Ø²Ù† Ø§Ù„ÙØ§Ø¦Ù‚ (Impossible CartPole)
# Ø§Ù„ØµØ¹ÙˆØ¨Ø©: ÙØ§Ø¦Ù‚Ø© Ø§Ù„ØµØ¹ÙˆØ¨Ø©
# 
# Ø§Ø³Ù… Ø§Ù„Ù…Ù„Ù: impossible_cartpole_challenge.py
# ØªØ§Ø±ÙŠØ® Ø§Ù„Ø¥Ù†Ø´Ø§Ø¡: 2025
# ========================================================================

"""
 ÙˆØµÙ Ø§Ù„ØªØ­Ø¯ÙŠ:
--------------
Ù†Ø³Ø®Ø© Ù…Ø¹Ø¯Ù‘Ù„Ø© ÙˆØ´Ø¯ÙŠØ¯Ø© Ø§Ù„ØµØ¹ÙˆØ¨Ø© Ù…Ù† CartPole Ø§Ù„ÙƒÙ„Ø§Ø³ÙŠÙƒÙŠ.
Ø¹Ø±Ø¨Ø© Ù…Ø¹ Ø¹Ù…ÙˆØ¯ ÙŠØ¬Ø¨ Ù…ÙˆØ§Ø²Ù†ØªÙ‡ ÙÙŠ Ø¸Ø±ÙˆÙ Ù‚Ø§Ø³ÙŠØ©:
- Ø¹Ù…ÙˆØ¯ Ø£Ø·ÙˆÙ„ ÙˆØ£Ø«Ù‚Ù„
- Ø­Ø±ÙƒØ© Ø£Ø³Ø±Ø¹
- Ù‡ÙˆØ§Ù…Ø´ Ø®Ø·Ø£ Ø£Ø¶ÙŠÙ‚
- Ø±ÙŠØ§Ø­ Ø¹Ø´ÙˆØ§Ø¦ÙŠØ© ÙˆÙ…ÙØ§Ø¬Ø¦Ø©
- Ø§Ø­ØªÙƒØ§Ùƒ ØºÙŠØ± Ù…Ù†ØªØ¸Ù…

 Ø§Ù„Ù‚ÙˆØ§Ù†ÙŠÙ† ÙˆØ§Ù„Ù‚ÙŠÙˆØ¯:
-------------------
1. ÙŠØ¬Ø¨ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© Q-Learning ÙÙ‚Ø·
2. Ù„Ø§ ÙŠØ³Ù…Ø­ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Deep Learning Ø£Ùˆ Neural Networks
3. Ø§Ù„Ø­Ø§Ù„Ø©: 4 Ø£Ø¨Ø¹Ø§Ø¯ (Ù…ÙˆÙ‚Ø¹ØŒ Ø³Ø±Ø¹Ø©ØŒ Ø²Ø§ÙˆÙŠØ©ØŒ Ø³Ø±Ø¹Ø© Ø²Ø§ÙˆÙŠØ©)
4. Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª: ÙŠØ³Ø§Ø± (0) Ø£Ùˆ ÙŠÙ…ÙŠÙ† (1)
5. Ø§Ù„Ù†Ø¬Ø§Ø­ = Ø§Ù„Ø¨Ù‚Ø§Ø¡ Ù…ØªÙˆØ§Ø²Ù†Ø§Ù‹ Ù„Ø£Ø·ÙˆÙ„ ÙØªØ±Ø© Ù…Ù…ÙƒÙ†Ø©

Ø§Ù„ØªØ¹Ø¯ÙŠÙ„Ø§Øª Ø§Ù„ØµØ¹Ø¨Ø©:
- Ø·ÙˆÙ„ Ø§Ù„Ø¹Ù…ÙˆØ¯: 2Ã— Ø§Ù„Ø·ÙˆÙ„ Ø§Ù„Ø¹Ø§Ø¯ÙŠ
- ÙƒØªÙ„Ø© Ø§Ù„Ø¹Ù…ÙˆØ¯: 3Ã— Ø§Ù„ÙƒØªÙ„Ø© Ø§Ù„Ø¹Ø§Ø¯ÙŠØ©
- Ù‚ÙˆØ© Ø§Ù„Ø¯ÙØ¹: 50% Ù…Ù† Ø§Ù„Ù‚ÙˆØ© Ø§Ù„Ø¹Ø§Ø¯ÙŠØ©
- Ø²Ø§ÙˆÙŠØ© Ø§Ù„Ø³Ù‚ÙˆØ·: Â±8Â° Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Â±12Â°
- Ø±ÙŠØ§Ø­ Ø¹Ø´ÙˆØ§Ø¦ÙŠØ© ÙƒÙ„ 10-30 Ø®Ø·ÙˆØ©
- Ø§Ø­ØªÙƒØ§Ùƒ Ù…ØªØºÙŠØ±

 Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„ØªÙ‚ÙŠÙŠÙ…:
------------------
- Ø§Ù„Ù‡Ø¯Ù: Ø§Ù„Ø¨Ù‚Ø§Ø¡ Ù„Ø£ÙƒØ«Ø± Ù…Ù† 500 Ø®Ø·ÙˆØ© (ØµØ¹Ø¨ Ø¬Ø¯Ø§Ù‹)
- Ø§Ù„Ù…ØªÙˆØ³Ø· Ø§Ù„Ø¬ÙŠØ¯: > 200 Ø®Ø·ÙˆØ©
- Ø§Ù„Ù…ØªÙˆØ³Ø· Ø§Ù„Ø¹Ø§Ø¯ÙŠ: 50-100 Ø®Ø·ÙˆØ©
- Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ: Ù…ØªÙˆØ³Ø· Ø¢Ø®Ø± 100 Ø­Ù„Ù‚Ø©

 ØªÙ†Ø¨ÙŠÙ‡Ø§Øª Ù‡Ø§Ù…Ø©:
-----------------
- Ù‡Ø°Ø§ Ø§Ù„ØªØ­Ø¯ÙŠ Ù…Ø³ØªØ­ÙŠÙ„ ØªÙ‚Ø±ÙŠØ¨Ø§Ù‹
- ØµÙÙ…Ù… Ù„ÙŠÙƒÙˆÙ† Ø¹Ù„Ù‰ Ø­Ø§ÙØ© Ù‚Ø¯Ø±Ø§Øª Q-Learning
- ÙŠØ­ØªØ§Ø¬ Ø¶Ø¨Ø· Ø¯Ù‚ÙŠÙ‚ Ø¬Ø¯Ø§Ù‹ Ù„Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª
- Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª ØªØ¨Ø³ÙŠØ· Ø§Ù„Ø­Ø§Ù„Ø© Ø­Ø±Ø¬Ø©
- Ø§Ù„ØªÙˆÙ‚Ø¹Ø§Øª: Ø§Ù„Ù†Ø¬Ø§Ø­ Ø§Ù„Ø¬Ø²Ø¦ÙŠ ÙÙ‚Ø·
"""

# ========================================================================
# 1ï¸âƒ£ Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
# ========================================================================

import numpy as np
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation
from IPython.display import HTML
import gymnasium as gym
from gymnasium import spaces
from collections import defaultdict
import warnings
warnings.filterwarnings('ignore')

# ========================================================================
# 2ï¸âƒ£ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¨ÙŠØ¦Ø© (Ù„Ø§ ØªØ¹Ø¯Ù„ Ù‡Ø°Ø§ Ø§Ù„Ù‚Ø³Ù…!)
# ========================================================================

class ImpossibleCartPoleEnv(gym.Env):
    """
    Ù†Ø³Ø®Ø© Ù…Ø³ØªØ­ÙŠÙ„Ø© Ù…Ù† CartPole
     Ù…Ù…Ù†ÙˆØ¹ Ø§Ù„ØªØ¹Ø¯ÙŠÙ„ Ø¹Ù„Ù‰ Ù‡Ø°Ø§ Ø§Ù„ÙƒÙ„Ø§Ø³!
    
    Ø§Ù„ØªØ¹Ø¯ÙŠÙ„Ø§Øª Ø§Ù„ØµØ¹Ø¨Ø©:
    - Ø¹Ù…ÙˆØ¯ Ø£Ø·ÙˆÙ„ ÙˆØ£Ø«Ù‚Ù„ (ØµØ¹Ø¨ Ø§Ù„Ù…ÙˆØ§Ø²Ù†Ø©)
    - Ù‚ÙˆØ© Ø¯ÙØ¹ Ø£Ù‚Ù„ (Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø¨Ø·ÙŠØ¦Ø©)
    - Ø²ÙˆØ§ÙŠØ§ Ø³Ù‚ÙˆØ· Ø£Ø¶ÙŠÙ‚
    - Ø±ÙŠØ§Ø­ Ø¹Ø´ÙˆØ§Ø¦ÙŠØ©
    - Ø§Ø­ØªÙƒØ§Ùƒ Ù…ØªØºÙŠØ±
    """
    
    def __init__(self):
        super(ImpossibleCartPoleEnv, self).__init__()
        
        # Ù…Ø¹Ø§Ù…Ù„Ø§Øª ÙÙŠØ²ÙŠØ§Ø¦ÙŠØ© ØµØ¹Ø¨Ø©
        self.gravity = 9.8
        self.masscart = 1.0
        self.masspole = 0.3  # 3Ã— Ø§Ù„ÙˆØ²Ù† Ø§Ù„Ø¹Ø§Ø¯ÙŠ!
        self.total_mass = self.masspole + self.masscart
        self.length = 1.0  # 2Ã— Ø§Ù„Ø·ÙˆÙ„ Ø§Ù„Ø¹Ø§Ø¯ÙŠ!
        self.polemass_length = self.masspole * self.length
        self.force_mag = 5.0  # 50% Ù…Ù† Ø§Ù„Ù‚ÙˆØ© Ø§Ù„Ø¹Ø§Ø¯ÙŠØ©!
        self.tau = 0.02
        
        # Ø­Ø¯ÙˆØ¯ Ø£Ø¶ÙŠÙ‚
        self.theta_threshold_radians = 8 * 2 * np.pi / 360  # Â±8Â° ÙÙ‚Ø·!
        self.x_threshold = 2.4
        
        # Ø±ÙŠØ§Ø­ Ø¹Ø´ÙˆØ§Ø¦ÙŠØ©
        self.wind_force = 0
        self.wind_counter = 0
        self.wind_interval = np.random.randint(10, 30)
        
        # Ø§Ø­ØªÙƒØ§Ùƒ Ù…ØªØºÙŠØ±
        self.friction = 0.1
        
        # Ø­Ø¯ÙˆØ¯ Ø§Ù„Ø­Ø§Ù„Ø©
        high = np.array([
            self.x_threshold * 2,
            np.finfo(np.float32).max,
            self.theta_threshold_radians * 2,
            np.finfo(np.float32).max
        ], dtype=np.float32)
        
        self.action_space = spaces.Discrete(2)
        self.observation_space = spaces.Box(-high, high, dtype=np.float32)
        
        self.state = None
        self.steps_beyond_done = None
    
    def reset(self, seed=None, options=None):
        """Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ† Ø§Ù„Ø¨ÙŠØ¦Ø©"""
        super().reset(seed=seed)
        
        # Ø­Ø§Ù„Ø© Ø§Ø¨ØªØ¯Ø§Ø¦ÙŠØ© Ø¹Ø´ÙˆØ§Ø¦ÙŠØ© (Ø£ØµØ¹Ø¨)
        self.state = np.random.uniform(low=-0.1, high=0.1, size=(4,))
        self.steps_beyond_done = None
        self.wind_force = 0
        self.wind_counter = 0
        self.wind_interval = np.random.randint(10, 30)
        
        return np.array(self.state, dtype=np.float32), {}
    
    def step(self, action):
        """ØªÙ†ÙÙŠØ° Ø®Ø·ÙˆØ© ÙÙŠ Ø§Ù„Ø¨ÙŠØ¦Ø©"""
        assert self.action_space.contains(action)
        
        x, x_dot, theta, theta_dot = self.state
        
        # Ø§Ù„Ù‚ÙˆØ© Ø§Ù„Ù…Ø·Ø¨Ù‚Ø©
        force = self.force_mag if action == 1 else -self.force_mag
        
        # Ø±ÙŠØ§Ø­ Ø¹Ø´ÙˆØ§Ø¦ÙŠØ©
        self.wind_counter += 1
        if self.wind_counter >= self.wind_interval:
            self.wind_force = np.random.uniform(-3, 3)
            self.wind_counter = 0
            self.wind_interval = np.random.randint(10, 30)
        
        force += self.wind_force
        
        # Ø§Ø­ØªÙƒØ§Ùƒ Ù…ØªØºÙŠØ±
        self.friction = 0.05 + 0.1 * np.random.random()
        force -= self.friction * x_dot
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„ÙÙŠØ²ÙŠØ§Ø¡
        costheta = np.cos(theta)
        sintheta = np.sin(theta)
        
        temp = (force + self.polemass_length * theta_dot ** 2 * sintheta) / self.total_mass
        thetaacc = (self.gravity * sintheta - costheta * temp) / \
                   (self.length * (4.0 / 3.0 - self.masspole * costheta ** 2 / self.total_mass))
        xacc = temp - self.polemass_length * thetaacc * costheta / self.total_mass
        
        # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø­Ø§Ù„Ø©
        x = x + self.tau * x_dot
        x_dot = x_dot + self.tau * xacc
        theta = theta + self.tau * theta_dot
        theta_dot = theta_dot + self.tau * thetaacc
        
        self.state = (x, x_dot, theta, theta_dot)
        
        # Ø´Ø±ÙˆØ· Ø§Ù„ÙØ´Ù„ Ø§Ù„ØµØ§Ø±Ù…Ø©
        done = bool(
            x < -self.x_threshold
            or x > self.x_threshold
            or theta < -self.theta_threshold_radians
            or theta > self.theta_threshold_radians
        )
        
        if not done:
            reward = 1.0
        elif self.steps_beyond_done is None:
            self.steps_beyond_done = 0
            reward = 1.0
        else:
            self.steps_beyond_done += 1
            reward = 0.0
        
        return np.array(self.state, dtype=np.float32), reward, done, False, {}


class ImpossibleCartPoleChallenge:
    """
    ÙˆØ§Ø¬Ù‡Ø© ØªØ­Ø¯ÙŠ Impossible CartPole
     Ù…Ù…Ù†ÙˆØ¹ Ø§Ù„ØªØ¹Ø¯ÙŠÙ„!
    """
    
    def __init__(self):
        self.env = ImpossibleCartPoleEnv()
        self.state_bins = None
    
    def setup_discretization(self, bins_per_dimension=20):
        """
        Ø¥Ø¹Ø¯Ø§Ø¯ ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø­Ø§Ù„Ø§Øª
        
        Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª:
        -----------
        bins_per_dimension: Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ù‚Ø³Ø§Ù… Ù„ÙƒÙ„ Ø¨ÙØ¹Ø¯
        
         Ù†ØµÙŠØ­Ø©: Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø£Ø¹Ù„Ù‰ = Ø¯Ù‚Ø© Ø£ÙƒØ«Ø± Ù„ÙƒÙ† ØªØ¯Ø±ÙŠØ¨ Ø£Ø¨Ø·Ø£
        """
        # Ø­Ø¯ÙˆØ¯ Ø¯Ù‚ÙŠÙ‚Ø© Ù„ÙƒÙ„ Ø¨ÙØ¹Ø¯
        self.state_bounds = [
            (-2.4, 2.4),      # x position
            (-3.0, 3.0),      # x velocity
            (-0.21, 0.21),    # theta (Â±12Â°)
            (-3.0, 3.0)       # theta velocity
        ]
        
        self.state_bins = []
        for low, high in self.state_bounds:
            self.state_bins.append(
                np.linspace(low, high, bins_per_dimension)
            )
    
    def discretize_state(self, state):
        """ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø­Ø§Ù„Ø© Ø§Ù„Ù…Ø³ØªÙ…Ø±Ø© Ø¥Ù„Ù‰ Ù…Ù†ÙØµÙ„Ø©"""
        discrete_state = []
        
        for i, (s, bins) in enumerate(zip(state, self.state_bins)):
            s_clipped = np.clip(s, self.state_bounds[i][0], 
                               self.state_bounds[i][1])
            idx = np.digitize(s_clipped, bins)
            discrete_state.append(idx)
        
        return tuple(discrete_state)
    
    def reset(self):
        """Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ† Ø§Ù„Ø¨ÙŠØ¦Ø©"""
        state, _ = self.env.reset()
        return self.discretize_state(state)
    
    def step(self, action):
        """ØªÙ†ÙÙŠØ° Ø®Ø·ÙˆØ©"""
        next_state, reward, done, truncated, info = self.env.step(action)
        return self.discretize_state(next_state), reward, done or truncated, info
    
    def render(self):
        """Ø¹Ø±Ø¶ Ø§Ù„Ø¨ÙŠØ¦Ø©"""
        return self.env.render()
    
    def close(self):
        """Ø¥ØºÙ„Ø§Ù‚ Ø§Ù„Ø¨ÙŠØ¦Ø©"""
        self.env.close()


# ========================================================================
# 3ï¸âƒ£ Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© Q-Learning (ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„ØªØ¹Ø¯ÙŠÙ„ Ù‡Ù†Ø§!)
# ========================================================================

class QLearningAgent:
    """
    ÙˆÙƒÙŠÙ„ Q-Learning Ù„Ù„ØªØ­Ø¯ÙŠ Ø§Ù„Ù…Ø³ØªØ­ÙŠÙ„
    
     ÙŠÙ…ÙƒÙ†Ùƒ ØªØ¹Ø¯ÙŠÙ„:
    - Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª
    - Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ù…ØªÙ‚Ø¯Ù…Ø©
    - Ø£Ø³Ø§Ù„ÙŠØ¨ Ø§Ù„ØªØ¹Ù„Ù…
    
     Ù†ØµØ§Ø¦Ø­ Ù„Ù„Ù†Ø¬Ø§Ø­:
    - learning_rate Ø¹Ø§Ù„ÙŠ ÙÙŠ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ©
    - discount_factor Ù‚Ø±ÙŠØ¨ Ù…Ù† 1.0
    - epsilon_decay Ø¨Ø·ÙŠØ¡
    - Ø¬Ø±Ø¨ Optimistic Initialization
    """
    
    def __init__(self, 
                 n_actions=2,
                 learning_rate=0.5,
                 discount_factor=0.99,
                 epsilon_start=1.0,
                 epsilon_end=0.01,
                 epsilon_decay=0.9995,
                 optimistic_init=0.0):
        
        self.n_actions = n_actions
        self.learning_rate = learning_rate
        self.discount_factor = discount_factor
        self.epsilon = epsilon_start
        self.epsilon_end = epsilon_end
        self.epsilon_decay = epsilon_decay
        self.optimistic_init = optimistic_init
        
        # Ø¬Ø¯ÙˆÙ„ Q Ù…Ø¹ Ù‚ÙŠÙ… ØªÙØ§Ø¤Ù„ÙŠØ© Ø§Ø®ØªÙŠØ§Ø±ÙŠØ©
        self.q_table = defaultdict(
            lambda: np.ones(n_actions) * optimistic_init
        )
        
        self.training_episodes = 0
    
    def get_action(self, state, training=True):
        """Ø§Ø®ØªÙŠØ§Ø± Ø¥Ø¬Ø±Ø§Ø¡"""
        if training and np.random.random() < self.epsilon:
            return np.random.randint(0, self.n_actions)
        else:
            q_values = self.q_table[state]
            max_q = np.max(q_values)
            best_actions = np.where(q_values == max_q)[0]
            return np.random.choice(best_actions)
    
    def update(self, state, action, reward, next_state, done):
        """ØªØ­Ø¯ÙŠØ« Ø¬Ø¯ÙˆÙ„ Q"""
        current_q = self.q_table[state][action]
        
        if done:
            max_next_q = 0
        else:
            max_next_q = np.max(self.q_table[next_state])
        
        target_q = reward + self.discount_factor * max_next_q
        new_q = current_q + self.learning_rate * (target_q - current_q)
        self.q_table[state][action] = new_q
    
    def decay_epsilon(self):
        """ØªÙ‚Ù„ÙŠÙ„ epsilon"""
        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)
        self.training_episodes += 1


# ========================================================================
# 4ï¸âƒ£ Ø¯Ø§Ù„Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨
# ========================================================================

def train_impossible_cartpole(agent, env, n_episodes=5000, verbose=True):
    """
    ØªØ¯Ø±ÙŠØ¨ Ø§Ù„ÙˆÙƒÙŠÙ„ Ø¹Ù„Ù‰ Ø§Ù„ØªØ­Ø¯ÙŠ Ø§Ù„Ù…Ø³ØªØ­ÙŠÙ„
    
     ØªØ­Ø°ÙŠØ±: ÙŠØ­ØªØ§Ø¬ ØªØ¯Ø±ÙŠØ¨ Ù…ÙƒØ«Ù
    """
    
    episode_rewards = []
    episode_lengths = []
    
    print(" Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¹Ù„Ù‰  CartPole...")
    print("=" * 70)
    print(" Ø§Ù„ØªØ­Ø¯ÙŠ Ø§Ù„Ø£ØµØ¹Ø¨  !")
    print("   â€¢ Ø¹Ù…ÙˆØ¯ Ø£Ø·ÙˆÙ„ ÙˆØ£Ø«Ù‚Ù„")
    print("   â€¢ Ù‚ÙˆØ© Ø¯ÙØ¹ Ø£Ù‚Ù„")
    print("   â€¢ Ø±ÙŠØ§Ø­ Ø¹Ø´ÙˆØ§Ø¦ÙŠØ©")
    print("   â€¢ Ù‡ÙˆØ§Ù…Ø´ Ø®Ø·Ø£ Ø¶ÙŠÙ‚Ø© Ø¬Ø¯Ø§Ù‹")
    print("=" * 70)
    
    best_score = 0
    
    for episode in range(n_episodes):
        state = env.reset()
        episode_reward = 0
        steps = 0
        
        while True:
            action = agent.get_action(state, training=True)
            next_state, reward, done, info = env.step(action)
            
            agent.update(state, action, reward, next_state, done)
            
            episode_reward += reward
            steps += 1
            state = next_state
            
            if done:
                break
        
        agent.decay_epsilon()
        
        episode_rewards.append(episode_reward)
        episode_lengths.append(steps)
        
        if episode_reward > best_score:
            best_score = episode_reward
        
        if verbose and (episode + 1) % 100 == 0:
            avg_reward = np.mean(episode_rewards[-100:])
            avg_length = np.mean(episode_lengths[-100:])
            
            status = "ğŸ”´"
            if avg_reward > 200:
                status = "ğŸŸ¢ Ù…Ù…ØªØ§Ø²!"
            elif avg_reward > 100:
                status = "ğŸŸ¡ Ø¬ÙŠØ¯"
            
            print(f"Ø§Ù„Ø­Ù„Ù‚Ø© {episode + 1:5d} | "
                  f"Ù…ØªÙˆØ³Ø·: {avg_reward:6.1f} | "
                  f"Ø£ÙØ¶Ù„: {best_score:6.0f} | "
                  f"Epsilon: {agent.epsilon:.3f} {status}")
    
    print("=" * 70)
    print(" Ø§ÙƒØªÙ…Ù„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨!")
    print(f"   Ø£ÙØ¶Ù„ Ø£Ø¯Ø§Ø¡: {best_score:.0f} Ø®Ø·ÙˆØ©")
    
    return episode_rewards, episode_lengths


# ========================================================================
# 5ï¸âƒ£ Ø¯ÙˆØ§Ù„ Ø§Ù„ØªØµÙˆØ± ÙˆØ§Ù„ØªÙ‚ÙŠÙŠÙ…
# ========================================================================

def plot_training_results(episode_rewards, episode_lengths):
    """Ø±Ø³Ù… Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ¯Ø±ÙŠØ¨"""
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    fig.suptitle('ğŸ“Š Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ - Impossible CartPole', 
                 fontsize=16, weight='bold')
    
    # Ù…Ù†Ø­Ù†Ù‰ Ø§Ù„Ù…ÙƒØ§ÙØ¢Øª
    ax1 = axes[0, 0]
    ax1.plot(episode_rewards, alpha=0.2, color='blue')
    
    # Ø®Ø·ÙˆØ· Ø§Ù„Ø£Ù‡Ø¯Ø§Ù
    ax1.axhline(y=500, color='gold', linestyle='--', linewidth=2, 
                alpha=0.7, label='Ù…Ø³ØªØ­ÙŠÙ„ (500)')
    ax1.axhline(y=200, color='green', linestyle='--', linewidth=2, 
                alpha=0.7, label='Ù…Ù…ØªØ§Ø² (200)')
    ax1.axhline(y=100, color='orange', linestyle='--', linewidth=2, 
                alpha=0.7, label='Ø¬ÙŠØ¯ (100)')
    
    window = 100
    if len(episode_rewards) >= window:
        moving_avg = np.convolve(episode_rewards, 
                                np.ones(window)/window, mode='valid')
        ax1.plot(range(window-1, len(episode_rewards)), 
                moving_avg, color='red', linewidth=3, label='Ø§Ù„Ù…ØªÙˆØ³Ø·')
    
    ax1.set_xlabel('Ø±Ù‚Ù… Ø§Ù„Ø­Ù„Ù‚Ø©')
    ax1.set_ylabel('Ø¹Ø¯Ø¯ Ø§Ù„Ø®Ø·ÙˆØ§Øª')
    ax1.set_title('Ù…Ù†Ø­Ù†Ù‰ Ø§Ù„ØªØ¹Ù„Ù…')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # Ø£Ø·ÙˆØ§Ù„ Ø§Ù„Ø­Ù„Ù‚Ø§Øª (Ù†ÙØ³ Ø§Ù„Ø´ÙŠØ¡)
    ax2 = axes[0, 1]
    ax2.plot(episode_lengths, alpha=0.2, color='green')
    
    if len(episode_lengths) >= window:
        moving_avg = np.convolve(episode_lengths, 
                                np.ones(window)/window, mode='valid')
        ax2.plot(range(window-1, len(episode_lengths)), 
                moving_avg, color='orange', linewidth=3)
    
    ax2.set_xlabel('Ø±Ù‚Ù… Ø§Ù„Ø­Ù„Ù‚Ø©')
    ax2.set_ylabel('Ø¹Ø¯Ø¯ Ø§Ù„Ø®Ø·ÙˆØ§Øª')
    ax2.set_title('Ø§Ø³ØªÙ‚Ø±Ø§Ø± Ø§Ù„Ø£Ø¯Ø§Ø¡')
    ax2.grid(True, alpha=0.3)
    
    # ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø£Ø¯Ø§Ø¡
    ax3 = axes[1, 0]
    last_500 = episode_rewards[-500:]
    ax3.hist(last_500, bins=50, color='purple', alpha=0.7, edgecolor='black')
    ax3.axvline(np.mean(last_500), color='red', linestyle='--', 
                linewidth=3, label=f'Ø§Ù„Ù…ØªÙˆØ³Ø·: {np.mean(last_500):.1f}')
    ax3.axvline(np.median(last_500), color='blue', linestyle='--', 
                linewidth=2, label=f'Ø§Ù„ÙˆØ³ÙŠØ·: {np.median(last_500):.1f}')
    ax3.set_xlabel('Ø¹Ø¯Ø¯ Ø§Ù„Ø®Ø·ÙˆØ§Øª')
    ax3.set_ylabel('Ø§Ù„ØªÙƒØ±Ø§Ø±')
    ax3.set_title('ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø£Ø¯Ø§Ø¡ (Ø¢Ø®Ø± 500 Ø­Ù„Ù‚Ø©)')
    ax3.legend()
    ax3.grid(True, alpha=0.3, axis='y')
    
    # Ø§Ù„Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ù‚ÙŠØ§Ø³ÙŠØ©
    ax4 = axes[1, 1]
    records = []
    current_best = 0
    for reward in episode_rewards:
        if reward > current_best:
            current_best = reward
        records.append(current_best)
    
    ax4.plot(records, color='gold', linewidth=3)
    ax4.fill_between(range(len(records)), records, alpha=0.3, color='gold')
    ax4.set_xlabel('Ø±Ù‚Ù… Ø§Ù„Ø­Ù„Ù‚Ø©')
    ax4.set_ylabel('Ø§Ù„Ø±Ù‚Ù… Ø§Ù„Ù‚ÙŠØ§Ø³ÙŠ')
    ax4.set_title('ØªØ·ÙˆØ± Ø§Ù„Ø±Ù‚Ù… Ø§Ù„Ù‚ÙŠØ§Ø³ÙŠ')
    ax4.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()


def evaluate_agent(agent, env, n_episodes=100):
    """ØªÙ‚ÙŠÙŠÙ… Ø§Ù„ÙˆÙƒÙŠÙ„ Ø§Ù„Ù…Ø¯Ø±Ø¨"""
    
    print("\n" + "=" * 70)
    print("ğŸ“ˆ Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ...")
    print("=" * 70)
    
    episode_rewards = []
    
    for episode in range(n_episodes):
        state = env.reset()
        episode_reward = 0
        
        while True:
            action = agent.get_action(state, training=False)
            next_state, reward, done, _ = env.step(action)
            
            episode_reward += reward
            state = next_state
            
            if done:
                break
        
        episode_rewards.append(episode_reward)
    
    stats = {
        'Ù…ØªÙˆØ³Ø·_Ø§Ù„Ø®Ø·ÙˆØ§Øª': np.mean(episode_rewards),
        'Ø§Ù†Ø­Ø±Ø§Ù_Ù…Ø¹ÙŠØ§Ø±ÙŠ': np.std(episode_rewards),
        'Ø£ÙØ¶Ù„_Ø£Ø¯Ø§Ø¡': np.max(episode_rewards),
        'Ø£Ø³ÙˆØ£_Ø£Ø¯Ø§Ø¡': np.min(episode_rewards),
        'Ø§Ù„ÙˆØ³ÙŠØ·': np.median(episode_rewards),
        'Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹_Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ': np.sum(episode_rewards)
    }
    
    print(f"\n Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¹Ù„Ù‰ {n_episodes} Ø­Ù„Ù‚Ø©:")
    print(f"   â€¢ Ù…ØªÙˆØ³Ø· Ø§Ù„Ø®Ø·ÙˆØ§Øª: {stats['Ù…ØªÙˆØ³Ø·_Ø§Ù„Ø®Ø·ÙˆØ§Øª']:.1f} Â± {stats['Ø§Ù†Ø­Ø±Ø§Ù_Ù…Ø¹ÙŠØ§Ø±ÙŠ']:.1f}")
    print(f"   â€¢ Ø§Ù„ÙˆØ³ÙŠØ·: {stats['Ø§Ù„ÙˆØ³ÙŠØ·']:.1f}")
    print(f"   â€¢ Ø£ÙØ¶Ù„ Ø£Ø¯Ø§Ø¡: {stats['Ø£ÙØ¶Ù„_Ø£Ø¯Ø§Ø¡']:.0f}")
    print(f"   â€¢ Ø£Ø³ÙˆØ£ Ø£Ø¯Ø§Ø¡: {stats['Ø£Ø³ÙˆØ£_Ø£Ø¯Ø§Ø¡']:.0f}")
    print(f"\n Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ: {stats['Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹_Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ']:.0f}")
    
    # ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù…Ø³ØªÙˆÙ‰
    avg = stats['Ù…ØªÙˆØ³Ø·_Ø§Ù„Ø®Ø·ÙˆØ§Øª']
    print("\n  Ø§Ù„ØªÙ‚ÙŠÙŠÙ…:")
    if avg >= 500:
        print("   â­â­â­â­â­ Ù…Ø³ØªØ­ÙŠÙ„! Ø­Ù‚Ù‚Øª Ø§Ù„Ù…Ø³ØªØ­ÙŠÙ„!")
    elif avg >= 300:
        print("   â­â­â­â­ Ø§Ø³ØªØ«Ù†Ø§Ø¦ÙŠ! Ø£Ø¯Ø§Ø¡ Ø®Ø§Ø±Ù‚!")
    elif avg >= 200:
        print("   â­â­â­ Ù…Ù…ØªØ§Ø²! Ù†ØªÙŠØ¬Ø© Ø±Ø§Ø¦Ø¹Ø©!")
    elif avg >= 100:
        print("   â­â­ Ø¬ÙŠØ¯ Ø¬Ø¯Ø§Ù‹! Ø£Ø¯Ø§Ø¡ Ù‚ÙˆÙŠ")
    elif avg >= 50:
        print("   â­ Ù…Ù‚Ø¨ÙˆÙ„ - ÙŠÙ…ÙƒÙ† Ø§Ù„ØªØ­Ø³ÙŠÙ†")
    else:
        print("   ğŸ’ª Ø§Ø³ØªÙ…Ø± ÙÙŠ Ø§Ù„ØªØ¯Ø±ÙŠØ¨!")
    
    print("=" * 70)
    
    return stats


# ========================================================================
# 6ï¸âƒ£ Ø§Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
# ========================================================================

def main():
    """Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ Ù„Ù„ØªØ­Ø¯ÙŠ"""
    
    print("\n" + "=" * 70)
    print(" Impossible CartPole - Ø§Ù„Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø®Ø§Ù…Ø³ (ÙØ§Ø¦Ù‚ Ø§Ù„ØµØ¹ÙˆØ¨Ø©)")
    print("=" * 70)
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø¨ÙŠØ¦Ø©
    env = ImpossibleCartPoleChallenge()
    env.setup_discretization(bins_per_dimension=20)
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ÙˆÙƒÙŠÙ„ Ø¨Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ù…Ø­Ø³Ù‘Ù†Ø©
    agent = QLearningAgent(
        n_actions=2,
        learning_rate=0.3,          # Ø¬Ø±Ø¨: 0.2-0.5
        discount_factor=0.99,        # Ø¬Ø±Ø¨: 0.95-0.999
        epsilon_start=1.0,
        epsilon_end=0.01,
        epsilon_decay=0.9995,        # Ø¬Ø±Ø¨: 0.995-0.9999
        optimistic_init=0.0          # Ø¬Ø±Ø¨: 0, 5, 10
    )
    
    print("\nâš™ï¸  Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„ØªØ¹Ù„Ù…:")
    print(f"   â€¢ Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù…: {agent.learning_rate}")
    print(f"   â€¢ Ù…Ø¹Ø§Ù…Ù„ Ø§Ù„Ø®ØµÙ…: {agent.discount_factor}")
    print(f"   â€¢ Epsilon decay: {agent.epsilon_decay}")
    print(f"   â€¢ Optimistic init: {agent.optimistic_init}")
    
    print("\n Ù†ØµØ§Ø¦Ø­ Ù„Ù„ÙÙˆØ²:")
    print("   - Ø²Ø¯ Ø¹Ø¯Ø¯ Ø§Ù„Ø­Ù„Ù‚Ø§Øª (5000-10000)")
    print("   - Ø¬Ø±Ø¨ learning_rate Ø£Ø¹Ù„Ù‰ (0.3-0.5)")
    print("   - Ø¬Ø±Ø¨ epsilon_decay Ø£Ø¨Ø·Ø£ (0.999)")
    print("   - Ø¬Ø±Ø¨ optimistic initialization")
    
    # Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…ÙƒØ«Ù
    episode_rewards, episode_lengths = train_impossible_cartpole(
        agent, env, 
        n_episodes=5000,
        verbose=True
    )
    
    # Ø±Ø³Ù… Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    plot_training_results(episode_rewards, episode_lengths)
    
    # Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
    final_stats = evaluate_agent(agent, env, n_episodes=100)
    
    # Ø¥ØºÙ„Ø§Ù‚ Ø§Ù„Ø¨ÙŠØ¦Ø©
    env.close()
    
    return agent, env, final_stats


# ========================================================================
# ğŸš€ ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ­Ø¯ÙŠ
# ========================================================================

if __name__ == "__main__":
    agent, env, stats = main()
    
    print("\n Ø§Ù†ØªÙ‡Ù‰ Ø§Ù„ØªØ­Ø¯ÙŠ !")
    print("\n Ù…Ø§Ø°Ø§ ØªØ¹Ù„Ù…Ù†Ø§:")
    print("   - Q-Learning Ù‚ÙˆÙŠ Ù„ÙƒÙ† Ù„Ù‡ Ø­Ø¯ÙˆØ¯")
    print("   - Ø§Ù„Ø¶Ø¨Ø· Ø§Ù„Ø¯Ù‚ÙŠÙ‚ Ù„Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø­Ø§Ø³Ù…")
    print("   - Ø§Ù„ØªØ¨Ø³ÙŠØ· Ø§Ù„Ø°ÙƒÙŠ Ù…ÙØªØ§Ø­ Ø§Ù„Ù†Ø¬Ø§Ø­")
    print("   - Ø§Ù„ØµØ¨Ø± ÙˆØ§Ù„ØªØ¬Ø±ÙŠØ¨ Ø¶Ø±ÙˆØ±ÙŠØ§Ù†")
    print("\n ØªÙ‡Ø§Ù†ÙŠÙ†Ø§ Ø¹Ù„Ù‰ Ø¥ÙƒÙ…Ø§Ù„ Ø¬Ù…ÙŠØ¹ Ø§Ù„ØªØ­Ø¯ÙŠØ§Øª!")
