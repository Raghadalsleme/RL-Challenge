#!/usr/bin/env python3
# -*- coding: utf-8 -*-

# ========================================================================
# ุชุญุฏู ุงูุชุนูู ุงูุฐุงุชู (ูููุชุฎุตุตูู)
# ุงููุณุชูู ุงูุฑุงุจุน: ุงูุฑูุจูุช ุงููุงุดู (Bipedal Walker)
# ุงูุตุนูุจุฉ: ุฎุจูุฑ
# 
# ุงุณู ุงูููู: bipedal_walker_challenge.py
# ุชุงุฑูุฎ ุงูุฅูุดุงุก: 2025
# ========================================================================

"""
 ูุตู ุงูุชุญุฏู:
--------------
ุฑูุจูุช ุฐู ุณุงููู ูุญุงูู ุงููุดู ุนุจุฑ ุชุถุงุฑูุณ ูุนุฑุฉ.
ูุฌุจ ุงูุชุญูู ูู 4 ูุญุฑูุงุช (ููุงุตู) ูุชุญููู ุงููุดู ุงููุณุชูุฑ.

ุงูุจูุฆุฉ: BipedalWalker-v3
ุงููุฏู: ุงููุดู ูุฃุจุนุฏ ูุณุงูุฉ ููููุฉ ุฏูู ุงูุณููุท

 ุงูููุงููู ูุงููููุฏ:
-------------------
1. ูุฌุจ ุงุณุชุฎุฏุงู ุฎูุงุฑุฒููุฉ Q-Learning ููุท
2. ูุง ูุณูุญ ุจุงุณุชุฎุฏุงู Deep Learning ุฃู Neural Networks
3. ุงูุญุงูุฉ: 24 ุจูุนุฏ ูุณุชูุฑ (ุฒูุงูุงุ ุณุฑุนุงุชุ ููุงูุณุฉ ุงูุฃุฑุถ...)
4. ุงูุฅุฌุฑุงุกุงุช: 4 ููู ูุณุชูุฑุฉ ุจูู -1 ู +1 ููู ููุตู
5. ุงููุฌุงุญ = ููุงูุฃุฉ > 300 (ุงููุดู ุงููุงุฌุญ)

ุงูุญุงูุฉ (24 ุจูุนุฏ):
- ุณุฑุนุฉ ุงููููู ุงูุฃูููุฉ ูุงูุนููุฏูุฉ
- ุณุฑุนุฉ ุงูุฏูุฑุงู ุงูุฒุงููุฉ
- ุฒูุงูุง ุงูููุงุตู (4 ููุงุตู)
- ุณุฑุนุงุช ุงูููุงุตู ุงูุฒุงููุฉ
- ูุนูููุงุช ุงููุงูุณุฉ ููุฃุฑุถ (ููุงุท LIDAR - 10 ููุงุท)
- ููุงูุณุฉ ุงููุฏููู ููุฃุฑุถ

 ูุนุงููุฑ ุงูุชูููู:
------------------
- ุงููุดู ููุฃูุงู: +1 ููู ุฅุทุงุฑ
- ุงุณุชุฎุฏุงู ุงููุญุฑูุงุช: -0.00035 ููู ูุญุฑู
- ุงูุณููุท: -100
- ุงูููุงูุฃุฉ ุงููุณุชูุฏูุฉ: > 300
- ุงููุฌููุน ุงูููุงุฆู: ูุชูุณุท ุขุฎุฑ 100 ุญููุฉ

 ุชูุจููุงุช ูุงูุฉ:
-----------------
- ูุฐุง ุงูุชุญุฏู ููุฎุจุฑุงุก ููุท
- Q-Learning ุงูุชูููุฏู ุบูุฑ ููุงุณุจ ุชูุงูุงู
- ุงูุญุงูุฉ ูุงูุฅุฌุฑุงุกุงุช ุนุงููุฉ ุงูุฃุจุนุงุฏ
- ูุญุชุงุฌ ุงุณุชุฑุงุชูุฌูุงุช ุชุจุณูุท ุฅุจุฏุงุนูุฉ ุฌุฏุงู

"""

# ========================================================================
# 1๏ธโฃ ุงุณุชูุฑุงุฏ ุงูููุชุจุงุช ุงููุทููุจุฉ
# ========================================================================

import numpy as np
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation
from IPython.display import HTML
import gymnasium as gym
from collections import defaultdict
import warnings
warnings.filterwarnings('ignore')

# ========================================================================
# 2๏ธโฃ ุฅุนุฏุงุฏ ุงูุจูุฆุฉ (ูุง ุชุนุฏู ูุฐุง ุงููุณู!)
# ========================================================================

class BipedalWalkerChallenge:
    """
    ุจูุฆุฉ ุชุญุฏู Bipedal Walker
     ููููุน ุงูุชุนุฏูู ุนูู ูุฐุง ุงูููุงุณ!
    
    ุงูุชุญุฏู:
    - 24 ุจูุนุฏ ูุณุชูุฑ ููุญุงูุฉ
    - 4 ุฅุฌุฑุงุกุงุช ูุณุชูุฑุฉ
    - ููุฒูุงุก ูุนูุฏุฉ
    - ูุญุชุงุฌ ุชูุงุฒู ุฏููู
    """
    
    def __init__(self):
        # ุงุณุชุฎุฏุงู ุงูุฅุตุฏุงุฑ ุงููููุตู ููุชุจุณูุท
        self.env = gym.make('BipedalWalker-v3')
        
        # ุญุฏูุฏ ุชูุฑูุจูุฉ ููุญุงูุงุช
        self.state_bounds = [
            (-5, 5),    # hull angle speed
            (-5, 5),    # hull angular velocity
        ] + [(-3, 3)] * 4  # joint angles
        
        self.n_actions_per_joint = 3  # 3 ุฎูุงุฑุงุช ููู ููุตู
        self.total_actions = self.n_actions_per_joint ** 4  # 81 ุฅุฌุฑุงุก
        
    def discretize_state(self, state):
        """
        ุชุจุณูุท ุงูุญุงูุฉ ุนุงููุฉ ุงูุฃุจุนุงุฏ
        
        ุงูุงุณุชุฑุงุชูุฌูุฉ:
        - ุงุณุชุฎุฏุงู ุฃูู 6 ุฃุจุนุงุฏ ููุท
        - ุชุฌุงูู ูุนูููุงุช LIDAR (ูุจุณุทุฉ ุฌุฏุงู)
        - ุงูุชุฑููุฒ ุนูู ุงูุฒูุงูุง ูุงูุณุฑุนุงุช ุงูุฃุณุงุณูุฉ
        """
        # ุงุณุชุฎุฑุงุฌ ุฃูู ุงููุนูููุงุช
        hull_angle = state[0]
        hull_angular_vel = state[1]
        hip1_angle = state[4]
        knee1_angle = state[5]
        hip2_angle = state[8]
        knee2_angle = state[9]
        
        # ุชูุณูู ูู ุจูุนุฏ ุฅูู 5 ูุฆุงุช
        discretized = []
        values = [hull_angle, hull_angular_vel, hip1_angle, 
                 knee1_angle, hip2_angle, knee2_angle]
        
        for i, val in enumerate(values):
            if i < len(self.state_bounds):
                low, high = self.state_bounds[i]
            else:
                low, high = -3, 3
            
            # ุชูุณูู ุฅูู 5 ูุฆุงุช
            normalized = (val - low) / (high - low)
            normalized = np.clip(normalized, 0, 1)
            category = int(normalized * 4)
            discretized.append(category)
        
        return tuple(discretized)
    
    def discretize_action(self, action_idx):
        """
        ุชุญููู ููุฑุณ ุงูุฅุฌุฑุงุก ุงููููุตู ุฅูู 4 ููู ูุณุชูุฑุฉ
        
        ูู ููุตู ูู 3 ุฎูุงุฑุงุช: -1, 0, +1
        81 ุฅุฌุฑุงุก ูููู (3^4)
        """
        actions = []
        remaining = action_idx
        
        for _ in range(4):
            action_val = remaining % self.n_actions_per_joint
            remaining //= self.n_actions_per_joint
            
            # ุชุญููู 0,1,2 ุฅูู -1,0,+1
            if action_val == 0:
                actions.append(-1.0)
            elif action_val == 1:
                actions.append(0.0)
            else:
                actions.append(1.0)
        
        return np.array(actions)
    
    def reset(self):
        """ุฅุนุงุฏุฉ ุชุนููู ุงูุจูุฆุฉ"""
        state, _ = self.env.reset()
        return self.discretize_state(state)
    
    def step(self, action_idx):
        """
        ุชูููุฐ ุฎุทูุฉ ูู ุงูุจูุฆุฉ
        
        ุงูููุงูุขุช (ูุง ูููู ุชุนุฏูููุง):
        - ุงูุชูุฏู ููุฃูุงู: +1 ููู ุฅุทุงุฑ
        - ุงุณุชุฎุฏุงู ุงููุญุฑูุงุช: -0.00035
        - ุงูุณููุท: -100
        """
        continuous_action = self.discretize_action(action_idx)
        
        next_state, reward, terminated, truncated, info = self.env.step(
            continuous_action
        )
        done = terminated or truncated
        
        return self.discretize_state(next_state), reward, done, info
    
    def render(self):
        """ุนุฑุถ ุงูุจูุฆุฉ"""
        return self.env.render()
    
    def close(self):
        """ุฅุบูุงู ุงูุจูุฆุฉ"""
        self.env.close()


# ========================================================================
# 3๏ธโฃ ุฎูุงุฑุฒููุฉ Q-Learning (ููููู ุงูุชุนุฏูู ููุง!)
# ========================================================================

class QLearningAgent:
    """
    ูููู Q-Learning ูุชุญุฏู Bipedal Walker
    
     ููููู ุชุนุฏูู:
    - ุงููุนุงููุงุช
    - ุงุณุชุฑุงุชูุฌูุงุช ุงูุงุณุชูุดุงู
    - ุทุฑููุฉ ูุนุงูุฌุฉ ุงูุญุงูุงุช
    
     ูุง ููููู:
    - ุงุณุชุฎุฏุงู Neural Networks
    
   
    """
    
    def __init__(self, 
                 n_actions=81,
                 learning_rate=0.1,
                 discount_factor=0.99,
                 epsilon_start=1.0,
                 epsilon_end=0.1,
                 epsilon_decay=0.9995):
        
        self.n_actions = n_actions
        self.learning_rate = learning_rate
        self.discount_factor = discount_factor
        self.epsilon = epsilon_start
        self.epsilon_end = epsilon_end
        self.epsilon_decay = epsilon_decay
        
        # ุฌุฏูู Q
        self.q_table = defaultdict(lambda: np.zeros(n_actions))
        
        # ุฅุญุตุงุฆูุงุช
        self.training_episodes = 0
    
    def get_action(self, state, training=True):
        """ุงุฎุชูุงุฑ ุฅุฌุฑุงุก"""
        if training and np.random.random() < self.epsilon:
            return np.random.randint(0, self.n_actions)
        else:
            q_values = self.q_table[state]
            max_q = np.max(q_values)
            best_actions = np.where(q_values == max_q)[0]
            return np.random.choice(best_actions)
    
    def update(self, state, action, reward, next_state, done):
        """ุชุญุฏูุซ ุฌุฏูู Q"""
        current_q = self.q_table[state][action]
        
        if done:
            max_next_q = 0
        else:
            max_next_q = np.max(self.q_table[next_state])
        
        target_q = reward + self.discount_factor * max_next_q
        new_q = current_q + self.learning_rate * (target_q - current_q)
        self.q_table[state][action] = new_q
    
    def decay_epsilon(self):
        """ุชูููู epsilon"""
        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)
        self.training_episodes += 1


# ========================================================================
# 4๏ธโฃ ุฏุงูุฉ ุงูุชุฏุฑูุจ
# ========================================================================

def train_bipedal_walker(agent, env, n_episodes=2000, max_steps=1600, verbose=True):
    """
    ุชุฏุฑูุจ ุงููููู ุนูู ุชุญุฏู Bipedal Walker
    
     ุชุญุฐูุฑ: ุงูุชุฏุฑูุจ ุทููู ุฌุฏุงู ูุงููุชุงุฆุฌ ูุญุฏูุฏุฉ
    """
    
    episode_rewards = []
    episode_lengths = []
    
    print(" ุจุฏุก ุงูุชุฏุฑูุจ ุนูู Bipedal Walker...")
    print("=" * 70)
    print("  ุชุญุฐูุฑ ุญุฑุฌ:")
    print("   ูุฐุง ุงูุชุญุฏู ุดุจู ูุณุชุญูู ูุน Q-Learning ุงูุชูููุฏู!")
    print("   ุงูููุงูุขุช ุงูุณูุจูุฉ ูุชููุนุฉ")
    print("   ุงููุฏู: ููู ุญุฏูุฏ ุงูุทุฑู ุงูููุงุณูููุฉ")
    print("=" * 70)
    
    for episode in range(n_episodes):
        state = env.reset()
        episode_reward = 0
        
        for step in range(max_steps):
            action = agent.get_action(state, training=True)
            next_state, reward, done, info = env.step(action)
            
            agent.update(state, action, reward, next_state, done)
            
            episode_reward += reward
            state = next_state
            
            if done:
                break
        
        agent.decay_epsilon()
        
        episode_rewards.append(episode_reward)
        episode_lengths.append(step + 1)
        
        if verbose and (episode + 1) % 100 == 0:
            avg_reward = np.mean(episode_rewards[-100:])
            avg_length = np.mean(episode_lengths[-100:])
            best_reward = max(episode_rewards[-100:])
            
            print(f"ุงูุญููุฉ {episode + 1:4d} | "
                  f"ูุชูุณุท ุงูููุงูุฃุฉ: {avg_reward:8.2f} | "
                  f"ุฃูุถู: {best_reward:8.2f} | "
                  f"ุงูุทูู: {avg_length:6.1f} | "
                  f"Epsilon: {agent.epsilon:.3f}")
    
    print("=" * 70)
    print(" ุงูุชูู ุงูุชุฏุฑูุจ!")
    
    return episode_rewards, episode_lengths


# ========================================================================
# 5๏ธโฃ ุฏูุงู ุงูุชุตูุฑ ูุงูุชูููู
# ========================================================================

def plot_training_results(episode_rewards, episode_lengths):
    """ุฑุณู ูุชุงุฆุฌ ุงูุชุฏุฑูุจ"""
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    fig.suptitle(' ูุชุงุฆุฌ ุงูุชุฏุฑูุจ - ุชุญุฏู Bipedal Walker', 
                 fontsize=16, weight='bold')
    
    # ููุญูู ุงูููุงูุขุช
    ax1 = axes[0, 0]
    ax1.plot(episode_rewards, alpha=0.3, color='blue')
    ax1.axhline(y=300, color='green', linestyle='--', 
                linewidth=2, label='ูุฏู ุงููุฌุงุญ (300)')
    
    window = 100
    if len(episode_rewards) >= window:
        moving_avg = np.convolve(episode_rewards, 
                                np.ones(window)/window, mode='valid')
        ax1.plot(range(window-1, len(episode_rewards)), 
                moving_avg, color='red', linewidth=2)
    
    ax1.set_xlabel('ุฑูู ุงูุญููุฉ')
    ax1.set_ylabel('ุงูููุงูุฃุฉ ุงููููุฉ')
    ax1.set_title('ููุญูู ุงูุชุนูู - ุงูููุงูุขุช')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # ุฃุทูุงู ุงูุญููุงุช
    ax2 = axes[0, 1]
    ax2.plot(episode_lengths, alpha=0.3, color='green')
    
    if len(episode_lengths) >= window:
        moving_avg = np.convolve(episode_lengths, 
                                np.ones(window)/window, mode='valid')
        ax2.plot(range(window-1, len(episode_lengths)), 
                moving_avg, color='orange', linewidth=2)
    
    ax2.set_xlabel('ุฑูู ุงูุญููุฉ')
    ax2.set_ylabel('ุนุฏุฏ ุงูุฎุทูุงุช')
    ax2.set_title('ุทูู ุงูุญููุงุช (ุฃุทูู = ุฃูุถู)')
    ax2.grid(True, alpha=0.3)
    
    # ุชูุฒูุน ุงูููุงูุขุช
    ax3 = axes[1, 0]
    last_200 = episode_rewards[-200:]
    ax3.hist(last_200, bins=40, color='purple', alpha=0.7, edgecolor='black')
    ax3.axvline(np.mean(last_200), color='red', linestyle='--', linewidth=2)
    ax3.set_xlabel('ุงูููุงูุฃุฉ')
    ax3.set_ylabel('ุงูุชูุฑุงุฑ')
    ax3.set_title('ุชูุฒูุน ุงูููุงูุขุช (ุขุฎุฑ 200 ุญููุฉ)')
    ax3.grid(True, alpha=0.3, axis='y')
    
    # ุงูุชุญุณู ุนุจุฑ ุงูููุช
    ax4 = axes[1, 1]
    improvement = []
    for i in range(100, len(episode_rewards), 50):
        improvement.append(np.mean(episode_rewards[i-100:i]))
    
    ax4.plot(range(100, len(episode_rewards), 50), improvement, 
            color='teal', linewidth=2, marker='o', markersize=4)
    ax4.set_xlabel('ุฑูู ุงูุญููุฉ')
    ax4.set_ylabel('ูุชูุณุท ุงูููุงูุฃุฉ')
    ax4.set_title('ุงูุชุญุณู ุงูุชุฏุฑูุฌู')
    ax4.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()


def evaluate_agent(agent, env, n_episodes=20):
    """ุชูููู ุงููููู ุงููุฏุฑุจ"""
    
    print("\n" + "=" * 70)
    print(" ุชูููู ุงูุฃุฏุงุก ุงูููุงุฆู...")
    print("=" * 70)
    
    episode_rewards = []
    episode_lengths = []
    
    for episode in range(n_episodes):
        state = env.reset()
        episode_reward = 0
        
        for step in range(1600):
            action = agent.get_action(state, training=False)
            next_state, reward, done, _ = env.step(action)
            
            episode_reward += reward
            state = next_state
            
            if done:
                break
        
        episode_rewards.append(episode_reward)
        episode_lengths.append(step + 1)
    
    stats = {
        'ูุชูุณุท_ุงูููุงูุฃุฉ': np.mean(episode_rewards),
        'ุงูุญุฑุงู_ูุนูุงุฑู_ุงูููุงูุฃุฉ': np.std(episode_rewards),
        'ุฃูุถู_ููุงูุฃุฉ': np.max(episode_rewards),
        'ุฃุณูุฃ_ููุงูุฃุฉ': np.min(episode_rewards),
        'ูุชูุณุท_ุงูุฎุทูุงุช': np.mean(episode_lengths),
        'ุงููุฌููุน_ุงูููุงุฆู': np.sum(episode_rewards)
    }
    
    print(f"\n ุงููุชุงุฆุฌ ุนูู {n_episodes} ุญููุฉ:")
    print(f"   โข ูุชูุณุท ุงูููุงูุฃุฉ: {stats['ูุชูุณุท_ุงูููุงูุฃุฉ']:.2f} ยฑ {stats['ุงูุญุฑุงู_ูุนูุงุฑู_ุงูููุงูุฃุฉ']:.2f}")
    print(f"   โข ุฃูุถู ููุงูุฃุฉ: {stats['ุฃูุถู_ููุงูุฃุฉ']:.2f}")
    print(f"   โข ุฃุณูุฃ ููุงูุฃุฉ: {stats['ุฃุณูุฃ_ููุงูุฃุฉ']:.2f}")
    print(f"   โข ูุชูุณุท ุนุฏุฏ ุงูุฎุทูุงุช: {stats['ูุชูุณุท_ุงูุฎุทูุงุช']:.1f}")
    print(f"\n  ุงููุฌููุน ุงูููุงุฆู ููููุงุท: {stats['ุงููุฌููุน_ุงูููุงุฆู']:.0f}")
    
    print("\n ุงููุงูุน:")
    if stats['ูุชูุณุท_ุงูููุงูุฃุฉ'] > -50:
        print("   ูุชูุฌุฉ ููุจููุฉ ูุธุฑุงู ูุตุนูุจุฉ ุงูุชุญุฏู!")
    else:
        print("   ุงููุชุงุฆุฌ ูุญุฏูุฏุฉ - ูุฐุง ูุชููุน ูุน Q-Learning")
    
    print("   ูุฐุง ุงูุชุญุฏู ูุญุชุงุฌ Deep RL (PPO, SAC, TD3)")
    print("   Q-Learning ุงูุชูููุฏู ุบูุฑ ููุงุณุจ ููุชุญูู ุงููุณุชูุฑ ุงููุนูุฏ")
    
    print("=" * 70)
    
    return stats


# ========================================================================
# 6๏ธโฃ ุงูุชุดุบูู ุงูุฑุฆูุณู
# ========================================================================

def main():
    """ุงูุจุฑูุงูุฌ ุงูุฑุฆูุณู ููุชุญุฏู"""
    
    print("\n" + "=" * 70)
    print(" ุชุญุฏู Bipedal Walker - ุงููุณุชูู ุงูุฑุงุจุน (ุฎุจูุฑ)")
    print("=" * 70)
    
    # ุฅูุดุงุก ุงูุจูุฆุฉ
    env = BipedalWalkerChallenge()
    
    # ุฅูุดุงุก ุงููููู
    agent = QLearningAgent(
        n_actions=81,
        learning_rate=0.2,
        discount_factor=0.98,
        epsilon_start=1.0,
        epsilon_end=0.1,
        epsilon_decay=0.9995
    )
    
    print("\nโ๏ธ  ูุนุงููุงุช ุงูุชุนูู ุงููุณุชุฎุฏูุฉ:")
    print(f"   โข ูุนุฏู ุงูุชุนูู (ฮฑ): {agent.learning_rate}")
    print(f"   โข ูุนุงูู ุงูุฎุตู (ฮณ): {agent.discount_factor}")
    print(f"   โข Epsilon ุงูููุงุฆู: {agent.epsilon_end}")
    print(f"   โข ุนุฏุฏ ุงูุฅุฌุฑุงุกุงุช: {agent.n_actions}")
    
    print("\n  ุชุญุฐูุฑ ููุฎุจุฑุงุก:")
    print("   ูุฐุง ุงูุชุญุฏู ูุตูู ูู Deep Reinforcement Learning")
    print("   Q-Learning ุงูุชูููุฏู ูู ูุญูู ูุชุงุฆุฌ ุฌูุฏุฉ")
    print("   ุงููุฏู ุงูุชุนูููู: ููู ูุชู ูุญุชุงุฌ Deep RL")
    
    # ุงูุชุฏุฑูุจ
    episode_rewards, episode_lengths = train_bipedal_walker(
        agent, env, 
        n_episodes=1000,
        max_steps=1600,
        verbose=True
    )
    
    # ุฑุณู ุงููุชุงุฆุฌ
    plot_training_results(episode_rewards, episode_lengths)
    
    # ุงูุชูููู ุงูููุงุฆู
    final_stats = evaluate_agent(agent, env, n_episodes=20)
    
    # ุฅุบูุงู ุงูุจูุฆุฉ
    env.close()
    
    return agent, env, final_stats


# ========================================================================
# ๐ ุชุดุบูู ุงูุชุญุฏู
# ========================================================================

if __name__ == "__main__":
    print("\n" + "=" * 70)
    print("  ุชุซุจูุช ุงูููุชุจุงุช ุงููุทููุจุฉ:")
    print("   pip install gymnasium")
    print("   pip install gymnasium[box2d]")
    print("=" * 70)
    
    agent, env, stats = main()
    
    print("\n ุงูุชูู ุงูุชุญุฏู!")
    print("\n ุฏุฑูุณ ูุณุชูุงุฏุฉ:")
    print("   - Q-Learning ุงูุชูููุฏู ูู ุญุฏูุฏ ูุงุถุญุฉ")
    print("   - ุงูุญุงูุงุช ูุงูุฅุฌุฑุงุกุงุช ุนุงููุฉ ุงูุฃุจุนุงุฏ ุชุญุชุงุฌ Deep RL")
    print("   - ุงูููุฒูุงุก ุงููุนูุฏุฉ ุชุญุชุงุฌ ุชูุฑูุจ ุฏูุงู ุงููููุฉ")
    print("   - PPO, SAC, TD3 ูู ุงูุฎูุงุฑุฒููุงุช ุงูููุงุณุจุฉ ููุง")
