# ========================================================================
# ØªØ­Ø¯ÙŠ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø°Ø§ØªÙŠ (Ù„Ù„Ù…ØªØ®ØµØµÙŠÙ†)
# Ø§Ù„Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø£ÙˆÙ„: ØªØ³Ù„Ù‚ Ø§Ù„Ø¬Ø¨Ù„ (Mountain Car)
# Ø§Ù„ØµØ¹ÙˆØ¨Ø©: Ø³Ù‡Ù„ (Ù…Ø¨ØªØ¯Ø¦)
# ========================================================================

"""
 ÙˆØµÙ Ø§Ù„ØªØ­Ø¯ÙŠ:
--------------
Ø³ÙŠØ§Ø±Ø© Ø¹Ø§Ù„Ù‚Ø© ÙÙŠ ÙˆØ§Ø¯Ù Ø¨ÙŠÙ† Ø¬Ø¨Ù„ÙŠÙ†. Ø§Ù„Ù‡Ø¯Ù Ù‡Ùˆ Ø§Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ Ø§Ù„Ø¹Ù„Ù… Ø¹Ù„Ù‰ Ù‚Ù…Ø© Ø§Ù„Ø¬Ø¨Ù„ Ø§Ù„Ø£ÙŠÙ…Ù†.
Ø§Ù„Ù…Ø´ÙƒÙ„Ø©: Ù…Ø­Ø±Ùƒ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø¶Ø¹ÙŠÙ ÙˆÙ„Ø§ ÙŠØ³ØªØ·ÙŠØ¹ Ø§Ù„ØµØ¹ÙˆØ¯ Ù…Ø¨Ø§Ø´Ø±Ø©!
Ø§Ù„Ø­Ù„: ÙŠØ¬Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø§Ù„ØªØ£Ø±Ø¬Ø­ Ø°Ù‡Ø§Ø¨Ø§Ù‹ ÙˆØ¥ÙŠØ§Ø¨Ø§Ù‹ Ù„Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø²Ø®Ù… ÙˆØ§Ù„ÙˆØµÙˆÙ„ Ù„Ù„Ù‚Ù…Ø©.

 Ø§Ù„Ù‚ÙˆØ§Ù†ÙŠÙ† ÙˆØ§Ù„Ù‚ÙŠÙˆØ¯:
-------------------
1. ÙŠØ¬Ø¨ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© Q-Learning ÙÙ‚Ø·
2. Ù„Ø§ ÙŠØ³Ù…Ø­ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Deep Learning Ø£Ùˆ Neural Networks
3. ÙŠØ¬Ø¨ ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø­Ø§Ù„Ø§Øª (State Discretization) Ù„Ø£Ù† Ø§Ù„ÙØ¶Ø§Ø¡ Ù…Ø³ØªÙ…Ø±
4. Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ù…Ø³Ù…ÙˆØ­Ø©: ÙŠØ³Ø§Ø± (0)ØŒ Ù„Ø§ Ø´ÙŠØ¡ (1)ØŒ ÙŠÙ…ÙŠÙ† (2)
5. Ø§Ù„Ù†Ø¬Ø§Ø­ = Ø§Ù„ÙˆØµÙˆÙ„ Ù„Ù„Ø¹Ù„Ù… ÙÙŠ Ø£Ù‚Ù„ Ù…Ù† 200 Ø®Ø·ÙˆØ©

ğŸ† Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„ØªÙ‚ÙŠÙŠÙ…:
------------------
- Ù†Ù‚Ø§Ø· Ø¥Ø¶Ø§ÙÙŠØ©: Ø§Ù„ÙˆØµÙˆÙ„ Ù„Ù„Ù‡Ø¯Ù ÙÙŠ Ø£Ù‚Ù„ Ø¹Ø¯Ø¯ Ø®Ø·ÙˆØ§Øª
- ÙŠØªÙ… Ø§Ø­ØªØ³Ø§Ø¨ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù…ØªÙˆØ³Ø· Ø¢Ø®Ø± 100 Ø­Ù„Ù‚Ø©
- Ø§Ù„ÙØ§Ø¦Ø²: Ø§Ù„ÙØ±ÙŠÙ‚ Ø§Ù„Ø°ÙŠ ÙŠØ­ØµÙ„ Ø¹Ù„Ù‰ Ø£Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹ ÙƒÙ„ÙŠ Ù„Ù„Ù†Ù‚Ø§Ø·

âš ï¸ ØªÙ†Ø¨ÙŠÙ‡Ø§Øª Ù‡Ø§Ù…Ø©:
-----------------
- Ù„Ø§ ØªÙ‚Ù… Ø¨ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ¦Ø© (Environment) Ø£Ùˆ Ù‚ÙˆØ§Ù†ÙŠÙ† Ø§Ù„Ù…ÙƒØ§ÙØ¢Øª
- ÙŠÙ…ÙƒÙ†Ùƒ ÙÙ‚Ø· ØªØ¹Ø¯ÙŠÙ„ Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„ØªØ¹Ù„Ù… ÙˆØ·Ø±ÙŠÙ‚Ø© ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø­Ø§Ù„Ø§Øª
- ÙŠØ¬Ø¨ Ø¹Ù„ÙŠÙƒ ØªÙ‚Ø¯ÙŠÙ… Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ù…ØµØ¯Ø±ÙŠ Ù…Ø¹ Ø´Ø±Ø­ Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ©
"""

# ========================================================================
# 1ï¸âƒ£ Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
# ========================================================================

import numpy as np
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation
from IPython.display import HTML
import gymnasium as gym
from collections import defaultdict

# ========================================================================
# 2ï¸âƒ£ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¨ÙŠØ¦Ø© (Ù„Ø§ ØªØ¹Ø¯Ù„ Ù‡Ø°Ø§ Ø§Ù„Ù‚Ø³Ù…!)
# ========================================================================

class MountainCarChallenge:
    """
    Ø¨ÙŠØ¦Ø© ØªØ­Ø¯ÙŠ Mountain Car
    âš ï¸ Ù…Ù…Ù†ÙˆØ¹ Ø§Ù„ØªØ¹Ø¯ÙŠÙ„ Ø¹Ù„Ù‰ Ù‡Ø°Ø§ Ø§Ù„ÙƒÙ„Ø§Ø³!
    """
    
    def __init__(self):
        self.env = gym.make('MountainCar-v0')
        self.position_bins = None
        self.velocity_bins = None
        
    def setup_discretization(self, n_position_bins=20, n_velocity_bins=20):
        """
        Ø¥Ø¹Ø¯Ø§Ø¯ ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„Ù…Ø³ØªÙ…Ø±Ø© Ø¥Ù„Ù‰ Ø­Ø§Ù„Ø§Øª Ù…Ù†ÙØµÙ„Ø©
        
        Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª:
        -----------
        n_position_bins: Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ù‚Ø³Ø§Ù… Ù„Ù„Ù…ÙˆÙ‚Ø¹
        n_velocity_bins: Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ù‚Ø³Ø§Ù… Ù„Ù„Ø³Ø±Ø¹Ø©
        """
        position_space = np.linspace(-1.2, 0.6, n_position_bins)
        velocity_space = np.linspace(-0.07, 0.07, n_velocity_bins)
        
        self.position_bins = position_space
        self.velocity_bins = velocity_space
    
    def discretize_state(self, state):
        """ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø­Ø§Ù„Ø© Ø§Ù„Ù…Ø³ØªÙ…Ø±Ø© Ø¥Ù„Ù‰ Ù…Ù†ÙØµÙ„Ø©"""
        position, velocity = state
        
        position_idx = np.digitize(position, self.position_bins)
        velocity_idx = np.digitize(velocity, self.velocity_bins)
        
        return (position_idx, velocity_idx)
    
    def reset(self):
        """Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ† Ø§Ù„Ø¨ÙŠØ¦Ø©"""
        state, _ = self.env.reset()
        return self.discretize_state(state)
    
    def step(self, action):
        """
        ØªÙ†ÙÙŠØ° Ø®Ø·ÙˆØ© ÙÙŠ Ø§Ù„Ø¨ÙŠØ¦Ø©
        
        Ø§Ù„Ù…ÙƒØ§ÙØ¢Øª (Ù„Ø§ ÙŠÙ…ÙƒÙ† ØªØ¹Ø¯ÙŠÙ„Ù‡Ø§):
        - ÙƒÙ„ Ø®Ø·ÙˆØ©: -1
        - Ø§Ù„ÙˆØµÙˆÙ„ Ù„Ù„Ù‡Ø¯Ù: 0 (ÙˆÙ„ÙƒÙ† ØªÙ†ØªÙ‡ÙŠ Ø§Ù„Ø­Ù„Ù‚Ø©)
        """
        next_state, reward, terminated, truncated, info = self.env.step(action)
        done = terminated or truncated
        
        return self.discretize_state(next_state), reward, done, info
    
    def render(self):
        """Ø¹Ø±Ø¶ Ø§Ù„Ø¨ÙŠØ¦Ø©"""
        return self.env.render()
    
    def close(self):
        """Ø¥ØºÙ„Ø§Ù‚ Ø§Ù„Ø¨ÙŠØ¦Ø©"""
        self.env.close()


# ========================================================================
# 3ï¸âƒ£ Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© Q-Learning (ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„ØªØ¹Ø¯ÙŠÙ„ Ù‡Ù†Ø§!)
# ========================================================================

class QLearningAgent:
    """
    ÙˆÙƒÙŠÙ„ Q-Learning Ù„Ù„ØªØ­Ø¯ÙŠ
    
     ÙŠÙ…ÙƒÙ†Ùƒ ØªØ¹Ø¯ÙŠÙ„:
    - Ù‚ÙŠÙ… Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª (learning_rate, discount_factor, etc.)
    - Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© epsilon decay
    - Ø·Ø±ÙŠÙ‚Ø© Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡
    
     Ù„Ø§ ÙŠÙ…ÙƒÙ†Ùƒ:
    - Ø§Ø³ØªØ®Ø¯Ø§Ù… Neural Networks
    - ØªØºÙŠÙŠØ± Ø§Ù„Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
    """
    
    def __init__(self, 
                 n_actions=3,
                 learning_rate=0.1,
                 discount_factor=0.99,
                 epsilon_start=1.0,
                 epsilon_end=0.01,
                 epsilon_decay=0.995):
        """
        Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„ØªØ¹Ø¯ÙŠÙ„:
        ---------------------------
        learning_rate: Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù… (alpha) - Ø¬Ø±Ø¨ Ù‚ÙŠÙ… Ø¨ÙŠÙ† 0.01 Ùˆ 0.5
        discount_factor: Ù…Ø¹Ø§Ù…Ù„ Ø§Ù„Ø®ØµÙ… (gamma) - Ø¬Ø±Ø¨ Ù‚ÙŠÙ… Ø¨ÙŠÙ† 0.9 Ùˆ 0.999
        epsilon_start: Ù‚ÙŠÙ…Ø© epsilon Ø§Ù„Ø§Ø¨ØªØ¯Ø§Ø¦ÙŠØ©
        epsilon_end: Ù‚ÙŠÙ…Ø© epsilon Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
        epsilon_decay: Ù…Ø¹Ø¯Ù„ ØªÙ†Ø§Ù‚Øµ epsilon
        """
        
        self.n_actions = n_actions
        self.learning_rate = learning_rate
        self.discount_factor = discount_factor
        self.epsilon = epsilon_start
        self.epsilon_end = epsilon_end
        self.epsilon_decay = epsilon_decay
        
        # Ø¬Ø¯ÙˆÙ„ Q (ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… defaultdict Ø£Ùˆ dict Ø¹Ø§Ø¯ÙŠ)
        self.q_table = defaultdict(lambda: np.zeros(n_actions))
    
    def get_action(self, state, training=True):
        """
        Ø§Ø®ØªÙŠØ§Ø± Ø¥Ø¬Ø±Ø§Ø¡ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… epsilon-greedy
        
        ÙŠÙ…ÙƒÙ†Ùƒ ØªØ¹Ø¯ÙŠÙ„ Ù‡Ø°Ù‡ Ø§Ù„Ø¯Ø§Ù„Ø© Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡!
        """
        if training and np.random.random() < self.epsilon:
            # Ø§Ø³ØªÙƒØ´Ø§Ù: Ø§Ø®ØªÙŠØ§Ø± Ø¹Ø´ÙˆØ§Ø¦ÙŠ
            return np.random.randint(0, self.n_actions)
        else:
            # Ø§Ø³ØªØºÙ„Ø§Ù„: Ø§Ø®ØªÙŠØ§Ø± Ø£ÙØ¶Ù„ Ø¥Ø¬Ø±Ø§Ø¡
            return np.argmax(self.q_table[state])
    
    def update(self, state, action, reward, next_state, done):
        """
        ØªØ­Ø¯ÙŠØ« Ø¬Ø¯ÙˆÙ„ Q
        
        ØµÙŠØºØ© Q-Learning:
        Q(s,a) = Q(s,a) + Î± * [r + Î³ * max(Q(s',a')) - Q(s,a)]
        """
        current_q = self.q_table[state][action]
        
        if done:
            # Ø¥Ø°Ø§ Ø§Ù†ØªÙ‡Øª Ø§Ù„Ø­Ù„Ù‚Ø©ØŒ Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ø­Ø§Ù„Ø© ØªØ§Ù„ÙŠØ©
            max_next_q = 0
        else:
            # Ø£Ù‚ØµÙ‰ Ù‚ÙŠÙ…Ø© Q Ù„Ù„Ø­Ø§Ù„Ø© Ø§Ù„ØªØ§Ù„ÙŠØ©
            max_next_q = np.max(self.q_table[next_state])
        
        # ØªØ­Ø¯ÙŠØ« Q
        new_q = current_q + self.learning_rate * (
            reward + self.discount_factor * max_next_q - current_q
        )
        
        self.q_table[state][action] = new_q
    
    def decay_epsilon(self):
        """ØªÙ‚Ù„ÙŠÙ„ epsilon Ø¨Ø¹Ø¯ ÙƒÙ„ Ø­Ù„Ù‚Ø©"""
        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)


# ========================================================================
# 4ï¸âƒ£ Ø¯Ø§Ù„Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨
# ========================================================================

def train_mountain_car(agent, env, n_episodes=1000, max_steps=200, verbose=True):
    """
    ØªØ¯Ø±ÙŠØ¨ Ø§Ù„ÙˆÙƒÙŠÙ„ Ø¹Ù„Ù‰ ØªØ­Ø¯ÙŠ Mountain Car
    
    Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª:
    -----------
    agent: ÙˆÙƒÙŠÙ„ Q-Learning
    env: Ø¨ÙŠØ¦Ø© Ø§Ù„ØªØ­Ø¯ÙŠ
    n_episodes: Ø¹Ø¯Ø¯ Ø§Ù„Ø­Ù„Ù‚Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ÙŠØ©
    max_steps: Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ù„Ø®Ø·ÙˆØ§Øª ÙÙŠ ÙƒÙ„ Ø­Ù„Ù‚Ø©
    verbose: Ø¹Ø±Ø¶ Ø§Ù„ØªÙ‚Ø¯Ù…
    
    Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª:
    ---------
    episode_rewards: Ù‚Ø§Ø¦Ù…Ø© Ø¨Ù…ÙƒØ§ÙØ¢Øª ÙƒÙ„ Ø­Ù„Ù‚Ø©
    episode_lengths: Ù‚Ø§Ø¦Ù…Ø© Ø¨Ø£Ø·ÙˆØ§Ù„ ÙƒÙ„ Ø­Ù„Ù‚Ø©
    """
    
    episode_rewards = []
    episode_lengths = []
    success_count = 0
    
    print(" Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨...")
    print("=" * 70)
    
    for episode in range(n_episodes):
        state = env.reset()
        episode_reward = 0
        
        for step in range(max_steps):
            # Ø§Ø®ØªÙŠØ§Ø± ÙˆØªÙ†ÙÙŠØ° Ø¥Ø¬Ø±Ø§Ø¡
            action = agent.get_action(state, training=True)
            next_state, reward, done, info = env.step(action)
            
            # ØªØ­Ø¯ÙŠØ« Q-table
            agent.update(state, action, reward, next_state, done)
            
            episode_reward += reward
            state = next_state
            
            if done:
                if step < max_steps - 1:  # Ù†Ø¬Ø­ ÙÙŠ Ø§Ù„ÙˆØµÙˆÙ„
                    success_count += 1
                break
        
        # ØªÙ‚Ù„ÙŠÙ„ epsilon
        agent.decay_epsilon()
        
        episode_rewards.append(episode_reward)
        episode_lengths.append(step + 1)
        
        # Ø¹Ø±Ø¶ Ø§Ù„ØªÙ‚Ø¯Ù…
        if verbose and (episode + 1) % 100 == 0:
            avg_reward = np.mean(episode_rewards[-100:])
            avg_length = np.mean(episode_lengths[-100:])
            success_rate = (success_count / 100) * 100
            
            print(f"Ø§Ù„Ø­Ù„Ù‚Ø© {episode + 1:4d} | "
                  f"Ù…ØªÙˆØ³Ø· Ø§Ù„Ù…ÙƒØ§ÙØ£Ø©: {avg_reward:7.2f} | "
                  f"Ù…ØªÙˆØ³Ø· Ø§Ù„Ø·ÙˆÙ„: {avg_length:5.1f} | "
                  f"Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù†Ø¬Ø§Ø­: {success_rate:5.1f}% | "
                  f"Epsilon: {agent.epsilon:.3f}")
            
            success_count = 0
    
    print("=" * 70)
    print(" Ø§ÙƒØªÙ…Ù„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨!")
    
    return episode_rewards, episode_lengths


# ========================================================================
# 5ï¸âƒ£ Ø¯ÙˆØ§Ù„ Ø§Ù„ØªØµÙˆØ± ÙˆØ§Ù„ØªÙ‚ÙŠÙŠÙ…
# ========================================================================

def plot_training_results(episode_rewards, episode_lengths):
    """Ø±Ø³Ù… Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ¯Ø±ÙŠØ¨"""
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    fig.suptitle(' Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ - ØªØ­Ø¯ÙŠ Mountain Car', 
                 fontsize=16, weight='bold')
    
    # 1. Ù…Ù†Ø­Ù†Ù‰ Ø§Ù„Ù…ÙƒØ§ÙØ¢Øª
    ax1 = axes[0, 0]
    ax1.plot(episode_rewards, alpha=0.3, color='blue', label='Ø§Ù„Ù…ÙƒØ§ÙØ£Ø©')
    
    window = 100
    if len(episode_rewards) >= window:
        moving_avg = np.convolve(episode_rewards, 
                                np.ones(window)/window, mode='valid')
        ax1.plot(range(window-1, len(episode_rewards)), 
                moving_avg, color='red', linewidth=2, 
                label=f'Ø§Ù„Ù…ØªÙˆØ³Ø· Ø§Ù„Ù…ØªØ­Ø±Ùƒ ({window})')
    
    ax1.set_xlabel('Ø±Ù‚Ù… Ø§Ù„Ø­Ù„Ù‚Ø©', fontsize=11)
    ax1.set_ylabel('Ø§Ù„Ù…ÙƒØ§ÙØ£Ø© Ø§Ù„ÙƒÙ„ÙŠØ©', fontsize=11)
    ax1.set_title('Ù…Ù†Ø­Ù†Ù‰ Ø§Ù„ØªØ¹Ù„Ù… - Ø§Ù„Ù…ÙƒØ§ÙØ¢Øª', fontsize=12, weight='bold')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # 2. Ø£Ø·ÙˆØ§Ù„ Ø§Ù„Ø­Ù„Ù‚Ø§Øª
    ax2 = axes[0, 1]
    ax2.plot(episode_lengths, alpha=0.3, color='green', label='Ø§Ù„Ø·ÙˆÙ„')
    
    if len(episode_lengths) >= window:
        moving_avg = np.convolve(episode_lengths, 
                                np.ones(window)/window, mode='valid')
        ax2.plot(range(window-1, len(episode_lengths)), 
                moving_avg, color='orange', linewidth=2, 
                label=f'Ø§Ù„Ù…ØªÙˆØ³Ø· Ø§Ù„Ù…ØªØ­Ø±Ùƒ ({window})')
    
    ax2.set_xlabel('Ø±Ù‚Ù… Ø§Ù„Ø­Ù„Ù‚Ø©', fontsize=11)
    ax2.set_ylabel('Ø¹Ø¯Ø¯ Ø§Ù„Ø®Ø·ÙˆØ§Øª', fontsize=11)
    ax2.set_title('Ø·ÙˆÙ„ Ø§Ù„Ø­Ù„Ù‚Ø§Øª (Ø£Ù‚Ù„ = Ø£ÙØ¶Ù„)', fontsize=12, weight='bold')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # 3. Ø§Ù„ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ù„Ù„Ù…ÙƒØ§ÙØ¢Øª
    ax3 = axes[1, 0]
    last_100 = episode_rewards[-100:]
    ax3.hist(last_100, bins=30, color='purple', alpha=0.7, edgecolor='black')
    ax3.axvline(np.mean(last_100), color='red', linestyle='--', 
                linewidth=2, label=f'Ø§Ù„Ù…ØªÙˆØ³Ø·: {np.mean(last_100):.1f}')
    ax3.set_xlabel('Ø§Ù„Ù…ÙƒØ§ÙØ£Ø©', fontsize=11)
    ax3.set_ylabel('Ø§Ù„ØªÙƒØ±Ø§Ø±', fontsize=11)
    ax3.set_title('ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù…ÙƒØ§ÙØ¢Øª (Ø¢Ø®Ø± 100 Ø­Ù„Ù‚Ø©)', fontsize=12, weight='bold')
    ax3.legend()
    ax3.grid(True, alpha=0.3, axis='y')
    
    # 4. Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù†Ø¬Ø§Ø­
    ax4 = axes[1, 1]
    success_threshold = -200  # Ù†Ø¬Ø­ Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù…ÙƒØ§ÙØ£Ø© Ø£ÙƒØ¨Ø± Ù…Ù† -200
    success_rates = []
    
    for i in range(100, len(episode_rewards), 10):
        recent = episode_rewards[i-100:i]
        success_rate = (np.array(recent) > success_threshold).mean() * 100
        success_rates.append(success_rate)
    
    ax4.plot(range(100, len(episode_rewards), 10), success_rates, 
            color='teal', linewidth=2, marker='o', markersize=3)
    ax4.set_xlabel('Ø±Ù‚Ù… Ø§Ù„Ø­Ù„Ù‚Ø©', fontsize=11)
    ax4.set_ylabel('Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù†Ø¬Ø§Ø­ (%)', fontsize=11)
    ax4.set_title('Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù†Ø¬Ø§Ø­ (Ø¢Ø®Ø± 100 Ø­Ù„Ù‚Ø©)', fontsize=12, weight='bold')
    ax4.grid(True, alpha=0.3)
    ax4.set_ylim(0, 105)
    
    plt.tight_layout()
    plt.show()


def evaluate_agent(agent, env, n_episodes=100):
    """
    ØªÙ‚ÙŠÙŠÙ… Ø§Ù„ÙˆÙƒÙŠÙ„ Ø§Ù„Ù…Ø¯Ø±Ø¨
    
    Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª:
    ---------
    dict: Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡
    """
    
    print("\n" + "=" * 70)
    print(" ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ...")
    print("=" * 70)
    
    episode_rewards = []
    episode_lengths = []
    success_count = 0
    
    for episode in range(n_episodes):
        state = env.reset()
        episode_reward = 0
        
        for step in range(200):
            action = agent.get_action(state, training=False)
            next_state, reward, done, _ = env.step(action)
            
            episode_reward += reward
            state = next_state
            
            if done:
                if step < 199:
                    success_count += 1
                break
        
        episode_rewards.append(episode_reward)
        episode_lengths.append(step + 1)
    
    # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
    stats = {
        'Ù…ØªÙˆØ³Ø·_Ø§Ù„Ù…ÙƒØ§ÙØ£Ø©': np.mean(episode_rewards),
        'Ø§Ù†Ø­Ø±Ø§Ù_Ù…Ø¹ÙŠØ§Ø±ÙŠ_Ø§Ù„Ù…ÙƒØ§ÙØ£Ø©': np.std(episode_rewards),
        'Ø£ÙØ¶Ù„_Ù…ÙƒØ§ÙØ£Ø©': np.max(episode_rewards),
        'Ø£Ø³ÙˆØ£_Ù…ÙƒØ§ÙØ£Ø©': np.min(episode_rewards),
        'Ù…ØªÙˆØ³Ø·_Ø§Ù„Ø®Ø·ÙˆØ§Øª': np.mean(episode_lengths),
        'Ù…Ø¹Ø¯Ù„_Ø§Ù„Ù†Ø¬Ø§Ø­_%': (success_count / n_episodes) * 100,
        'Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹_Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ': np.sum(episode_rewards)
    }
    
    # Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    print(f"\n Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¹Ù„Ù‰ {n_episodes} Ø­Ù„Ù‚Ø©:")
    print(f"   â€¢ Ù…ØªÙˆØ³Ø· Ø§Ù„Ù…ÙƒØ§ÙØ£Ø©: {stats['Ù…ØªÙˆØ³Ø·_Ø§Ù„Ù…ÙƒØ§ÙØ£Ø©']:.2f} Â± {stats['Ø§Ù†Ø­Ø±Ø§Ù_Ù…Ø¹ÙŠØ§Ø±ÙŠ_Ø§Ù„Ù…ÙƒØ§ÙØ£Ø©']:.2f}")
    print(f"   â€¢ Ø£ÙØ¶Ù„ Ù…ÙƒØ§ÙØ£Ø©: {stats['Ø£ÙØ¶Ù„_Ù…ÙƒØ§ÙØ£Ø©']:.2f}")
    print(f"   â€¢ Ø£Ø³ÙˆØ£ Ù…ÙƒØ§ÙØ£Ø©: {stats['Ø£Ø³ÙˆØ£_Ù…ÙƒØ§ÙØ£Ø©']:.2f}")
    print(f"   â€¢ Ù…ØªÙˆØ³Ø· Ø¹Ø¯Ø¯ Ø§Ù„Ø®Ø·ÙˆØ§Øª: {stats['Ù…ØªÙˆØ³Ø·_Ø§Ù„Ø®Ø·ÙˆØ§Øª']:.1f}")
    print(f"   â€¢ Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù†Ø¬Ø§Ø­: {stats['Ù…Ø¹Ø¯Ù„_Ø§Ù„Ù†Ø¬Ø§Ø­_%']:.1f}%")
    print(f"\n Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ù„Ù„Ù†Ù‚Ø§Ø·: {stats['Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹_Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ']:.0f}")
    print("=" * 70)
    
    return stats


# ========================================================================
# 6ï¸âƒ£ Ø§Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
# ========================================================================

def main():
    """Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ Ù„Ù„ØªØ­Ø¯ÙŠ"""
    
    print("\n" + "=" * 70)
    print("  ØªØ­Ø¯ÙŠ Mountain Car - Ø§Ù„Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø£ÙˆÙ„ (Ù…Ø¨ØªØ¯Ø¦)")
    print("=" * 70)
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø¨ÙŠØ¦Ø©
    env = MountainCarChallenge()
    
    # Ø¥Ø¹Ø¯Ø§Ø¯ ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø­Ø§Ù„Ø§Øª (ÙŠÙ…ÙƒÙ†Ùƒ ØªØ¹Ø¯ÙŠÙ„ Ù‡Ø°Ù‡ Ø§Ù„Ù‚ÙŠÙ…!)
    env.setup_discretization(n_position_bins=20, n_velocity_bins=20)
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ÙˆÙƒÙŠÙ„ (ÙŠÙ…ÙƒÙ†Ùƒ ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª!)
    agent = QLearningAgent(
        n_actions=3,
        learning_rate=0.1,        # Ø¬Ø±Ø¨: 0.05, 0.2, 0.5
        discount_factor=0.99,      # Ø¬Ø±Ø¨: 0.95, 0.99, 0.999
        epsilon_start=1.0,
        epsilon_end=0.01,
        epsilon_decay=0.995        # Ø¬Ø±Ø¨: 0.99, 0.999
    )
    
    print("\nâš™ï¸  Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©:")
    print(f"   â€¢ Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù… (Î±): {agent.learning_rate}")
    print(f"   â€¢ Ù…Ø¹Ø§Ù…Ù„ Ø§Ù„Ø®ØµÙ… (Î³): {agent.discount_factor}")
    print(f"   â€¢ Epsilon Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ: {agent.epsilon_end}")
    print(f"   â€¢ Ù…Ø¹Ø¯Ù„ ØªÙ†Ø§Ù‚Øµ Epsilon: {agent.epsilon_decay}")
    print(f"   â€¢ ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø­Ø§Ù„Ø§Øª: 20Ã—20")
    
    # Ø§Ù„ØªØ¯Ø±ÙŠØ¨
    episode_rewards, episode_lengths = train_mountain_car(
        agent, env, 
        n_episodes=1000,  # ÙŠÙ…ÙƒÙ†Ùƒ Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø¹Ø¯Ø¯ Ù„Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø£Ø·ÙˆÙ„
        max_steps=200,
        verbose=True
    )
    
    # Ø±Ø³Ù… Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    plot_training_results(episode_rewards, episode_lengths)
    
    # Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
    final_stats = evaluate_agent(agent, env, n_episodes=100)
    
    # Ø¥ØºÙ„Ø§Ù‚ Ø§Ù„Ø¨ÙŠØ¦Ø©
    env.close()
    
    return agent, env, final_stats


# ========================================================================
#  ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ­Ø¯ÙŠ
# ========================================================================

if __name__ == "__main__":
    agent, env, stats = main()
    
    print("\n Ø§Ù†ØªÙ‡Ù‰ Ø§Ù„ØªØ­Ø¯ÙŠ!")
    print(" Ù†ØµÙŠØ­Ø©: Ø¬Ø±Ø¨ ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª ÙÙŠ Ø§Ù„Ù‚Ø³Ù… 6ï¸âƒ£ Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡")
    print(" Ù„Ø§ ØªÙ†Ø³Ù ØªÙˆØ«ÙŠÙ‚ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØªÙƒ ÙˆØ§Ù„ØªØºÙŠÙŠØ±Ø§Øª Ø§Ù„ØªÙŠ Ø£Ø¬Ø±ÙŠØªÙ‡Ø§!")
